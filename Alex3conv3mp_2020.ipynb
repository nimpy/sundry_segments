{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os import listdir\n",
    "from os import system\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import imageio\n",
    "from skimage.measure import block_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 8\n",
    "nb_channels = 3\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/niaki/Code/ImageNet/tiny-imagenet-200'\n",
    "\n",
    "train_data_dir      = base_dir + '/tiny_train16'\n",
    "validation_data_dir = base_dir + '/tiny_validation16'\n",
    "test_data_dir       = base_dir + '/tiny_test16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data_cropped(dir_patches):\n",
    "    \"\"\"Load all the patches from dir_patches into tensors for training the autoencoder.\n",
    "    Return:\n",
    "        patches_in  -- tensor of stacked patches centre-cropped to 8x8    \n",
    "    \"\"\"\n",
    "    files_patches = listdir(dir_patches + '/class0')\n",
    "    files_patches.sort()\n",
    "    \n",
    "    patches_in = []\n",
    "    patches_out = []\n",
    "\n",
    "    \n",
    "    for file_patch in files_patches:\n",
    "        patch_in = imageio.imread(dir_patches + '/class0/' + file_patch)\n",
    "        patch_in = patch_in[4: 12, 4: 12, :]  # center-crop to 8x8\n",
    "#         patch_out = block_reduce(patch_in, (2, 2, 1), func=np.mean)  # downsample (mean-pool)\n",
    "        \n",
    "        patches_in.append(patch_in)\n",
    "#         patches_out.append(patch_out)\n",
    "        \n",
    "\n",
    "    patches_in = np.array(patches_in)\n",
    "    patches_in = patches_in.astype(np.float64) / 255\n",
    "#     patches_in = np.expand_dims(patches_in, -1)  # need this if grayscale\n",
    "    \n",
    "#     patches_out = np.array(patches_out)\n",
    "#     patches_out = patches_out.astype(np.float64) / 255\n",
    "#     patches_out = np.expand_dims(patches_out, -1)  # need this if grayscale\n",
    "        \n",
    "    print(\"in\", patches_in.shape)# , \"; out\", patches_out.shape)\n",
    "    \n",
    "    return patches_in#, patches_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in (157086, 8, 8, 3)\n",
      "in (3932, 8, 8, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = loading_data_cropped(train_data_dir)  # y_train\n",
    "x_validation = loading_data_cropped(validation_data_dir)  # y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loading_data(dir_patches):\n",
    "#     \"\"\"Load all the patches from dir_patches into tensors for training the autoencoder.\n",
    "#     Return:\n",
    "#         patches_in  -- tensor of stacked patches in their original shape, 16x16\n",
    "#         patches_out -- tensor of the original patches downsampled to 8x8\n",
    "    \n",
    "#     \"\"\"\n",
    "#     files_patches = listdir(dir_patches + '/class0')\n",
    "#     files_patches.sort()\n",
    "    \n",
    "#     patches_in = []\n",
    "#     patches_out = []\n",
    "\n",
    "    \n",
    "#     for file_patch in files_patches:\n",
    "#         patch_in = imageio.imread(dir_patches + '/class0/' + file_patch)\n",
    "        \n",
    "#         patch_out = block_reduce(patch_in, (2, 2, 1), func=np.mean)  # downsample (mean-pool)\n",
    "        \n",
    "#         patches_in.append(patch_in)\n",
    "#         patches_out.append(patch_out)\n",
    "        \n",
    "\n",
    "#     patches_in = np.array(patches_in)\n",
    "#     patches_in = patches_in.astype(np.float64) / 255\n",
    "# #     patches_in = np.expand_dims(patches_in, -1)  # need this if grayscale\n",
    "    \n",
    "#     patches_out = np.array(patches_out)\n",
    "#     patches_out = patches_out.astype(np.float64) / 255\n",
    "# #     patches_out = np.expand_dims(patches_out, -1)  # need this if grayscale\n",
    "        \n",
    "#     print(\"in\", patches_in.shape, \"; out\", patches_out.shape)\n",
    "    \n",
    "#     return patches_in, patches_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, _ = loading_data(train_data_dir)  # y_train\n",
    "# x_validation, _ = loading_data(validation_data_dir)  # y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = x_train\n",
    "y_validation = x_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_index = np.random.randint(x_train.shape[0]) #  5429\n",
    "# print(np.array(np.round(x_train[temp_index] * 255), dtype=np.uint8))\n",
    "plt.imshow(np.array(np.round(x_train[temp_index] * 255), dtype=np.uint8))\n",
    "plt.show()\n",
    "plt.imshow(np.array(np.round(y_train[temp_index] * 255), dtype=np.uint8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 8, 8, 3)]         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 8, 8, 32)          896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 1, 32)          9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 2, 32)          9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 32)          9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 3)           867       \n",
      "=================================================================\n",
      "Total params: 48,003\n",
      "Trainable params: 48,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (patch_size, patch_size, nb_channels)\n",
    "input_img = Input(shape=input_shape)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation=\"elu\", padding=\"same\")(input_img)\n",
    "x = Conv2D(32, (3, 3), activation=\"elu\", padding=\"same\")(x)\n",
    "x = Conv2D(32, (3, 3), activation=\"elu\", padding=\"same\")(x)\n",
    "x = MaxPool2D((2, 2), padding=\"same\")(x)\n",
    "x = MaxPool2D((2, 2), padding=\"same\")(x)\n",
    "encoded = MaxPool2D((2, 2), padding=\"same\")(x)\n",
    "\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation=\"elu\", padding=\"same\")(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation=\"elu\", padding=\"same\")(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation=\"elu\", padding=\"same\")(x)  # decoded = Conv2D(3, (3, 3), activation=\"elu\", padding=\"same\")(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(3, (3, 3), activation=\"elu\", padding=\"same\")(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7fdb18c75208>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fdb18b1db70>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fdb118ee1d0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fdb118ee860>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fdb118eea90>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fdb118eebe0>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fdb118eed30>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fdb118eee80>\n",
      "<tensorflow.python.keras.layers.convolutional.UpSampling2D object at 0x7fdb118f10b8>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fdb118f11d0>\n",
      "<tensorflow.python.keras.layers.convolutional.UpSampling2D object at 0x7fdb1922fe80>\n"
     ]
    }
   ],
   "source": [
    "# model_version_dwnsmpld_output = 'patch_desc_ae_20201026_14251116_alex_3conv3mp_2020_augm_elu_lastelu_dwnsmpl'\n",
    "# autoencoder_dwnsmpld_output = load_model(base_dir + '/' + model_version_dwnsmpld_output + '.h5')\n",
    "\n",
    "# for i in range(11):\n",
    "#     print(autoencoder_dwnsmpld_output.get_layer(index=i))\n",
    "#     autoencoder.get_layer(index=i).set_weights(autoencoder_dwnsmpld_output.get_layer(index=i).get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datagen = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
    "    width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
    "    horizontal_flip=False, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "# os.environ['WANDB_MODE'] = 'dryrun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.10.9 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.7<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">fresh-vortex-8</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/nimpy/patch-desc-ae\" target=\"_blank\">https://wandb.ai/nimpy/patch-desc-ae</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/nimpy/patch-desc-ae/runs/2olygd6h\" target=\"_blank\">https://wandb.ai/nimpy/patch-desc-ae/runs/2olygd6h</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20201105_101337-2olygd6h</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "  project=\"patch-desc-ae\",\n",
    "  config={\n",
    "    \"augmentation\": True,\n",
    "    \"elus\": True,\n",
    "    \"last_layer_activation\": \"elu\",\n",
    "    \"downsampling_output\": False,\n",
    "    \"optimizer\": \"adadelta\", \n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"epochs\": 1000,\n",
    "    \"patch_size\": 8}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/nimpy/patch-desc-ae/runs/2olygd6h?jupyter=true\" style=\"border:none;width:100%;height:420px\">\n",
       "                </iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.Run at 0x7f4b48124080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/niaki/Code/ImageNet/tiny-imagenet-200/weights_patch_desc_ae_20201105_1014078_alex_3conv3mp_2020_augm_elu_lastelu_NOTdwnsmpl\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 4909 steps, validate for 123 steps\n",
      "Epoch 1/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5230 - val_loss: 0.5187\n",
      "Epoch 2/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5229 - val_loss: 0.5188\n",
      "Epoch 3/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5188\n",
      "Epoch 4/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5186\n",
      "Epoch 5/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5186\n",
      "Epoch 6/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5186\n",
      "Epoch 7/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5187\n",
      "Epoch 8/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5184\n",
      "Epoch 9/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5187\n",
      "Epoch 10/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5183\n",
      "Epoch 11/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5185\n",
      "Epoch 12/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5229 - val_loss: 0.5188\n",
      "Epoch 13/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5229 - val_loss: 0.5185\n",
      "Epoch 14/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5186\n",
      "Epoch 15/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5187\n",
      "Epoch 16/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5188\n",
      "Epoch 17/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5187\n",
      "Epoch 18/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5229 - val_loss: 0.5183\n",
      "Epoch 19/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5184\n",
      "Epoch 20/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5184\n",
      "Epoch 21/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5187\n",
      "Epoch 22/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5184\n",
      "Epoch 23/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5228 - val_loss: 0.5185\n",
      "Epoch 24/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5186\n",
      "Epoch 25/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5229 - val_loss: 0.5185\n",
      "Epoch 26/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5184\n",
      "Epoch 27/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5228 - val_loss: 0.5187\n",
      "Epoch 28/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5188\n",
      "Epoch 29/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5183\n",
      "Epoch 30/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5186\n",
      "Epoch 31/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5184\n",
      "Epoch 32/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5228 - val_loss: 0.5186\n",
      "Epoch 33/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5185\n",
      "Epoch 34/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5185\n",
      "Epoch 35/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5184\n",
      "Epoch 36/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5185\n",
      "Epoch 37/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5184\n",
      "Epoch 38/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5185\n",
      "Epoch 39/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5228 - val_loss: 0.5186\n",
      "Epoch 40/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5184\n",
      "Epoch 41/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5186\n",
      "Epoch 42/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5227 - val_loss: 0.5186\n",
      "Epoch 43/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5182\n",
      "Epoch 44/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5228 - val_loss: 0.5186\n",
      "Epoch 45/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5228 - val_loss: 0.5183\n",
      "Epoch 46/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5184\n",
      "Epoch 47/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5185\n",
      "Epoch 48/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5185\n",
      "Epoch 49/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5182\n",
      "Epoch 50/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5183\n",
      "Epoch 51/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5184\n",
      "Epoch 52/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5228 - val_loss: 0.5184\n",
      "Epoch 53/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5183\n",
      "Epoch 54/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5183\n",
      "Epoch 55/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5186\n",
      "Epoch 56/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5184\n",
      "Epoch 57/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5184\n",
      "Epoch 58/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5227 - val_loss: 0.5186\n",
      "Epoch 59/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5186\n",
      "Epoch 60/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5184\n",
      "Epoch 61/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5184\n",
      "Epoch 62/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5185\n",
      "Epoch 63/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5186\n",
      "Epoch 64/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5185\n",
      "Epoch 65/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5186\n",
      "Epoch 66/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5185\n",
      "Epoch 67/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5227 - val_loss: 0.5183\n",
      "Epoch 68/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5186\n",
      "Epoch 69/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5226 - val_loss: 0.5187\n",
      "Epoch 70/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5187\n",
      "Epoch 71/500\n",
      "4909/4909 [==============================] - 47s 9ms/step - loss: 0.5226 - val_loss: 0.5186\n",
      "Epoch 72/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5182\n",
      "Epoch 73/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5182\n",
      "Epoch 74/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5183\n",
      "Epoch 75/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5182\n",
      "Epoch 77/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5182\n",
      "Epoch 78/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5183\n",
      "Epoch 79/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5181\n",
      "Epoch 80/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5184\n",
      "Epoch 81/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5184\n",
      "Epoch 82/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5184\n",
      "Epoch 83/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5184\n",
      "Epoch 84/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5180\n",
      "Epoch 85/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5183\n",
      "Epoch 86/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5186\n",
      "Epoch 87/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5183\n",
      "Epoch 88/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5185\n",
      "Epoch 89/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5182\n",
      "Epoch 90/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5183\n",
      "Epoch 91/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5182\n",
      "Epoch 92/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5182\n",
      "Epoch 93/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5184\n",
      "Epoch 94/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5182\n",
      "Epoch 95/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5184\n",
      "Epoch 96/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5181\n",
      "Epoch 97/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5183\n",
      "Epoch 98/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5184\n",
      "Epoch 99/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5182\n",
      "Epoch 100/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5180\n",
      "Epoch 101/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5181\n",
      "Epoch 102/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5226 - val_loss: 0.5183\n",
      "Epoch 103/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5184\n",
      "Epoch 104/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5182\n",
      "Epoch 105/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5180\n",
      "Epoch 106/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5182\n",
      "Epoch 107/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5182\n",
      "Epoch 108/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5183\n",
      "Epoch 109/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5183\n",
      "Epoch 110/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5182\n",
      "Epoch 111/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5184\n",
      "Epoch 112/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5186\n",
      "Epoch 113/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5185\n",
      "Epoch 114/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5182\n",
      "Epoch 115/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5187\n",
      "Epoch 116/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5182\n",
      "Epoch 117/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5183\n",
      "Epoch 118/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5184\n",
      "Epoch 119/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5224 - val_loss: 0.5182\n",
      "Epoch 120/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5182\n",
      "Epoch 121/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5181\n",
      "Epoch 122/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5184\n",
      "Epoch 123/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5225 - val_loss: 0.5181\n",
      "Epoch 124/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5224 - val_loss: 0.5183\n",
      "Epoch 125/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5179\n",
      "Epoch 126/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5224 - val_loss: 0.5180\n",
      "Epoch 127/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5180\n",
      "Epoch 128/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5224 - val_loss: 0.5183\n",
      "Epoch 129/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5178\n",
      "Epoch 130/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5180\n",
      "Epoch 131/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5225 - val_loss: 0.5180\n",
      "Epoch 132/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5179\n",
      "Epoch 133/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5182\n",
      "Epoch 134/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5180\n",
      "Epoch 135/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5180\n",
      "Epoch 136/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5180\n",
      "Epoch 137/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5181\n",
      "Epoch 138/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5178\n",
      "Epoch 139/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5182\n",
      "Epoch 140/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5182\n",
      "Epoch 141/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5180\n",
      "Epoch 142/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5224 - val_loss: 0.5182\n",
      "Epoch 143/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5183\n",
      "Epoch 144/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5184\n",
      "Epoch 145/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5181\n",
      "Epoch 146/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5223 - val_loss: 0.5183\n",
      "Epoch 147/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5179\n",
      "Epoch 148/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5184\n",
      "Epoch 149/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5179\n",
      "Epoch 150/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5181\n",
      "Epoch 151/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5182\n",
      "Epoch 152/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5180\n",
      "Epoch 153/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5181\n",
      "Epoch 154/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5181\n",
      "Epoch 155/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5184\n",
      "Epoch 156/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5182\n",
      "Epoch 157/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5181\n",
      "Epoch 158/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5179\n",
      "Epoch 159/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5180\n",
      "Epoch 160/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5179\n",
      "Epoch 161/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5224 - val_loss: 0.5181\n",
      "Epoch 162/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5180\n",
      "Epoch 163/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5182\n",
      "Epoch 164/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5180\n",
      "Epoch 165/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5182\n",
      "Epoch 166/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5178\n",
      "Epoch 167/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5181\n",
      "Epoch 168/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5180\n",
      "Epoch 169/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5177\n",
      "Epoch 170/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5180\n",
      "Epoch 171/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5180\n",
      "Epoch 172/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5179\n",
      "Epoch 173/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5183\n",
      "Epoch 174/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5181\n",
      "Epoch 175/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5180\n",
      "Epoch 176/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5180\n",
      "Epoch 177/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5179\n",
      "Epoch 178/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5180\n",
      "Epoch 179/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5179\n",
      "Epoch 180/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5180\n",
      "Epoch 181/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5182\n",
      "Epoch 182/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5179\n",
      "Epoch 183/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5179\n",
      "Epoch 184/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5178\n",
      "Epoch 185/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5181\n",
      "Epoch 186/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5180\n",
      "Epoch 187/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5223 - val_loss: 0.5179\n",
      "Epoch 188/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5180\n",
      "Epoch 189/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5182\n",
      "Epoch 190/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5181\n",
      "Epoch 191/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5179\n",
      "Epoch 192/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5179\n",
      "Epoch 193/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5181\n",
      "Epoch 194/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5181\n",
      "Epoch 195/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5177\n",
      "Epoch 196/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5182\n",
      "Epoch 197/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5179\n",
      "Epoch 198/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5179\n",
      "Epoch 199/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5182\n",
      "Epoch 200/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5179\n",
      "Epoch 201/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5178\n",
      "Epoch 202/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5180\n",
      "Epoch 203/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5179\n",
      "Epoch 204/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5179\n",
      "Epoch 205/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5222 - val_loss: 0.5181\n",
      "Epoch 206/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5180\n",
      "Epoch 207/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5178\n",
      "Epoch 208/500\n",
      "4909/4909 [==============================] - 47s 9ms/step - loss: 0.5222 - val_loss: 0.5179\n",
      "Epoch 209/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5222 - val_loss: 0.5175\n",
      "Epoch 210/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5178\n",
      "Epoch 211/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5178\n",
      "Epoch 212/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5180\n",
      "Epoch 213/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5181\n",
      "Epoch 214/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5180\n",
      "Epoch 215/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5178\n",
      "Epoch 216/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5222 - val_loss: 0.5177\n",
      "Epoch 217/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5223 - val_loss: 0.5181\n",
      "Epoch 218/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5180\n",
      "Epoch 219/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5177\n",
      "Epoch 220/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5178\n",
      "Epoch 221/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5180\n",
      "Epoch 222/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5182\n",
      "Epoch 223/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5180\n",
      "Epoch 224/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5181\n",
      "Epoch 225/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5177\n",
      "Epoch 226/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5178\n",
      "Epoch 227/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5180\n",
      "Epoch 228/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5179\n",
      "Epoch 229/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5178\n",
      "Epoch 230/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5180\n",
      "Epoch 231/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5178\n",
      "Epoch 232/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5177\n",
      "Epoch 233/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5178\n",
      "Epoch 234/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5180\n",
      "Epoch 235/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5178\n",
      "Epoch 236/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5181\n",
      "Epoch 237/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5181\n",
      "Epoch 238/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5178\n",
      "Epoch 239/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5221 - val_loss: 0.5178\n",
      "Epoch 240/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5178\n",
      "Epoch 241/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5176\n",
      "Epoch 242/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5179\n",
      "Epoch 243/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5178\n",
      "Epoch 244/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5177\n",
      "Epoch 245/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5177\n",
      "Epoch 246/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5222 - val_loss: 0.5179\n",
      "Epoch 247/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5182\n",
      "Epoch 248/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5221 - val_loss: 0.5178\n",
      "Epoch 249/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5182\n",
      "Epoch 250/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5178\n",
      "Epoch 251/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5180\n",
      "Epoch 252/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5180\n",
      "Epoch 253/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5178\n",
      "Epoch 254/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5180\n",
      "Epoch 255/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5181\n",
      "Epoch 256/500\n",
      "4909/4909 [==============================] - 47s 9ms/step - loss: 0.5221 - val_loss: 0.5179\n",
      "Epoch 257/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5221 - val_loss: 0.5179\n",
      "Epoch 258/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5177\n",
      "Epoch 259/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5221 - val_loss: 0.5179\n",
      "Epoch 260/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5179\n",
      "Epoch 261/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5178\n",
      "Epoch 262/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5177\n",
      "Epoch 263/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5180\n",
      "Epoch 264/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5179\n",
      "Epoch 265/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5176\n",
      "Epoch 266/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5178\n",
      "Epoch 267/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5181\n",
      "Epoch 268/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5178\n",
      "Epoch 269/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5178\n",
      "Epoch 270/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5180\n",
      "Epoch 271/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5176\n",
      "Epoch 272/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5177\n",
      "Epoch 273/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5179\n",
      "Epoch 274/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5180\n",
      "Epoch 275/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5179\n",
      "Epoch 276/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5180\n",
      "Epoch 277/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5176\n",
      "Epoch 278/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5221 - val_loss: 0.5180\n",
      "Epoch 279/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5177\n",
      "Epoch 280/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5176\n",
      "Epoch 281/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5179\n",
      "Epoch 282/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5179\n",
      "Epoch 283/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5177\n",
      "Epoch 284/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5176\n",
      "Epoch 285/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5220 - val_loss: 0.5179\n",
      "Epoch 286/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5181\n",
      "Epoch 287/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5178\n",
      "Epoch 288/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5179\n",
      "Epoch 289/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5221 - val_loss: 0.5181\n",
      "Epoch 290/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5179\n",
      "Epoch 291/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5178\n",
      "Epoch 292/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5177\n",
      "Epoch 293/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5179\n",
      "Epoch 294/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5179\n",
      "Epoch 295/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5178\n",
      "Epoch 296/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5176\n",
      "Epoch 297/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5177\n",
      "Epoch 298/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5176\n",
      "Epoch 299/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5221 - val_loss: 0.5179\n",
      "Epoch 300/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5176\n",
      "Epoch 301/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5177\n",
      "Epoch 302/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5220 - val_loss: 0.5179\n",
      "Epoch 303/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5178\n",
      "Epoch 304/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5176\n",
      "Epoch 305/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5178\n",
      "Epoch 306/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5177\n",
      "Epoch 307/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5177\n",
      "Epoch 308/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5177\n",
      "Epoch 309/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5178\n",
      "Epoch 310/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5181\n",
      "Epoch 311/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5179\n",
      "Epoch 312/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5176\n",
      "Epoch 313/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5178\n",
      "Epoch 314/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5176\n",
      "Epoch 315/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5175\n",
      "Epoch 316/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5175\n",
      "Epoch 317/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5179\n",
      "Epoch 318/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5178\n",
      "Epoch 319/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5178\n",
      "Epoch 320/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5177\n",
      "Epoch 321/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5178\n",
      "Epoch 322/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5178\n",
      "Epoch 323/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5182\n",
      "Epoch 324/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5179\n",
      "Epoch 325/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5220 - val_loss: 0.5175\n",
      "Epoch 326/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5177\n",
      "Epoch 327/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5179\n",
      "Epoch 328/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5220 - val_loss: 0.5177\n",
      "Epoch 329/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5178\n",
      "Epoch 330/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5177\n",
      "Epoch 331/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5219 - val_loss: 0.5179\n",
      "Epoch 332/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5178\n",
      "Epoch 333/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5179\n",
      "Epoch 334/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5181\n",
      "Epoch 335/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5177\n",
      "Epoch 336/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5178\n",
      "Epoch 337/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5179\n",
      "Epoch 338/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5177\n",
      "Epoch 339/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5178\n",
      "Epoch 340/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5174\n",
      "Epoch 341/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5219 - val_loss: 0.5180\n",
      "Epoch 342/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5179\n",
      "Epoch 343/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5179\n",
      "Epoch 344/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5178\n",
      "Epoch 345/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5176\n",
      "Epoch 346/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5178\n",
      "Epoch 347/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5220 - val_loss: 0.5176\n",
      "Epoch 348/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5175\n",
      "Epoch 349/500\n",
      "4909/4909 [==============================] - 47s 9ms/step - loss: 0.5219 - val_loss: 0.5176\n",
      "Epoch 350/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5177\n",
      "Epoch 351/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5180\n",
      "Epoch 352/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5177\n",
      "Epoch 353/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5180\n",
      "Epoch 354/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5176\n",
      "Epoch 355/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5177\n",
      "Epoch 356/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5177\n",
      "Epoch 357/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5179\n",
      "Epoch 358/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5176\n",
      "Epoch 359/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5178\n",
      "Epoch 360/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5177\n",
      "Epoch 361/500\n",
      "4909/4909 [==============================] - 47s 9ms/step - loss: 0.5219 - val_loss: 0.5178\n",
      "Epoch 362/500\n",
      "4909/4909 [==============================] - 47s 9ms/step - loss: 0.5219 - val_loss: 0.5178\n",
      "Epoch 363/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5173\n",
      "Epoch 364/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5178\n",
      "Epoch 365/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5175\n",
      "Epoch 366/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5177\n",
      "Epoch 367/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5175\n",
      "Epoch 368/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5178\n",
      "Epoch 369/500\n",
      "4909/4909 [==============================] - 47s 10ms/step - loss: 0.5219 - val_loss: 0.5176\n",
      "Epoch 370/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 371/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5176\n",
      "Epoch 372/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5176\n",
      "Epoch 373/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5179\n",
      "Epoch 374/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5176\n",
      "Epoch 375/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5176\n",
      "Epoch 376/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5176\n",
      "Epoch 377/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5175\n",
      "Epoch 378/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5177\n",
      "Epoch 379/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5176\n",
      "Epoch 380/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5174\n",
      "Epoch 381/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5178\n",
      "Epoch 382/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5219 - val_loss: 0.5174\n",
      "Epoch 383/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5176\n",
      "Epoch 384/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5177\n",
      "Epoch 385/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5175\n",
      "Epoch 386/500\n",
      "4909/4909 [==============================] - 47s 10ms/step - loss: 0.5219 - val_loss: 0.5177\n",
      "Epoch 387/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5176\n",
      "Epoch 388/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5177\n",
      "Epoch 389/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5177\n",
      "Epoch 390/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5175\n",
      "Epoch 391/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 392/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5176\n",
      "Epoch 393/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5176\n",
      "Epoch 394/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5177\n",
      "Epoch 395/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 396/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5179\n",
      "Epoch 397/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5176\n",
      "Epoch 398/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5175\n",
      "Epoch 399/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5174\n",
      "Epoch 400/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5219 - val_loss: 0.5176\n",
      "Epoch 401/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5176\n",
      "Epoch 402/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5175\n",
      "Epoch 403/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5175\n",
      "Epoch 404/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 405/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 406/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 407/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 408/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5178\n",
      "Epoch 409/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5177\n",
      "Epoch 410/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5177\n",
      "Epoch 411/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5219 - val_loss: 0.5174\n",
      "Epoch 412/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5173\n",
      "Epoch 413/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5175\n",
      "Epoch 414/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 415/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5175\n",
      "Epoch 416/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 417/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5175\n",
      "Epoch 418/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5175\n",
      "Epoch 419/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 420/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5176\n",
      "Epoch 421/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5173\n",
      "Epoch 422/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5177\n",
      "Epoch 423/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5173\n",
      "Epoch 424/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5173\n",
      "Epoch 425/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 426/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5176\n",
      "Epoch 427/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5175\n",
      "Epoch 428/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 429/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 430/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5172\n",
      "Epoch 431/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5177\n",
      "Epoch 432/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 433/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5173\n",
      "Epoch 434/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5176\n",
      "Epoch 435/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5177\n",
      "Epoch 436/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5176\n",
      "Epoch 437/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5175\n",
      "Epoch 438/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 439/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 440/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5173\n",
      "Epoch 441/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5177\n",
      "Epoch 442/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5178\n",
      "Epoch 443/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5175\n",
      "Epoch 444/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5175\n",
      "Epoch 445/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5175\n",
      "Epoch 446/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5176\n",
      "Epoch 447/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5176\n",
      "Epoch 448/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5176\n",
      "Epoch 449/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 450/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5173\n",
      "Epoch 451/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5174\n",
      "Epoch 452/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5176\n",
      "Epoch 453/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5175\n",
      "Epoch 454/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5176\n",
      "Epoch 455/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5175\n",
      "Epoch 456/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5175\n",
      "Epoch 457/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 458/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5177\n",
      "Epoch 459/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5177\n",
      "Epoch 460/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5174\n",
      "Epoch 461/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5174\n",
      "Epoch 462/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5175\n",
      "Epoch 463/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5173\n",
      "Epoch 464/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5175\n",
      "Epoch 465/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5177\n",
      "Epoch 466/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5217 - val_loss: 0.5176\n",
      "Epoch 467/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5173\n",
      "Epoch 468/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5177\n",
      "Epoch 469/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5175\n",
      "Epoch 470/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5174\n",
      "Epoch 471/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5172\n",
      "Epoch 472/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5174\n",
      "Epoch 473/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5174\n",
      "Epoch 474/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5175\n",
      "Epoch 475/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5173\n",
      "Epoch 476/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5176\n",
      "Epoch 477/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5217 - val_loss: 0.5177\n",
      "Epoch 478/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5174\n",
      "Epoch 479/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5174\n",
      "Epoch 480/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5218 - val_loss: 0.5173\n",
      "Epoch 481/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5174\n",
      "Epoch 482/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5176\n",
      "Epoch 483/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5174\n",
      "Epoch 484/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5175\n",
      "Epoch 485/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5175\n",
      "Epoch 486/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5176\n",
      "Epoch 487/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5173\n",
      "Epoch 488/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5174\n",
      "Epoch 489/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5216 - val_loss: 0.5174\n",
      "Epoch 490/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5173\n",
      "Epoch 491/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5176\n",
      "Epoch 492/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5217 - val_loss: 0.5174\n",
      "Epoch 493/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5174\n",
      "Epoch 494/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5175\n",
      "Epoch 495/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5174\n",
      "Epoch 496/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5175\n",
      "Epoch 497/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5175\n",
      "Epoch 498/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5175\n",
      "Epoch 499/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5174\n",
      "Epoch 500/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5217 - val_loss: 0.5176\n"
     ]
    }
   ],
   "source": [
    "%%wandb\n",
    "\n",
    "model_version = 'patch_desc_ae_' + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + '8_alex_3conv3mp_2020_augm_elu_lastelu_NOTdwnsmpl_500moreepochs'\n",
    "\n",
    "os.system('mkdir ' + base_dir + '/weights_' + model_version)\n",
    "print(base_dir + '/weights_' + model_version)\n",
    "\n",
    "# checkpointer = ModelCheckpoint(base_dir + '/weights' + model_version + '/weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "history_callback = autoencoder.fit(image_datagen.flow(x_train, y_train, batch_size),\n",
    "                epochs=500,#wandb.config.epochs,\n",
    "                validation_data=image_datagen.flow(x_validation, y_validation, batch_size),\n",
    "                callbacks=[WandbCallback(data_type=\"image\", predictions=1)]\n",
    "                )\n",
    "autoencoder.save(base_dir + '/' + model_version + '.h5')\n",
    "\n",
    "# autoencoder = load_model(base_dir + '/' + model_version + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.522978796924531,\n",
       "  0.522914021532996,\n",
       "  0.5229440340504885,\n",
       "  0.522898068028778,\n",
       "  0.5229217225748252,\n",
       "  0.5228991604117836,\n",
       "  0.5228810934010651,\n",
       "  0.5229112950266877,\n",
       "  0.5228765922809165,\n",
       "  0.5229317130214756,\n",
       "  0.5228847160859704,\n",
       "  0.5228936727290142,\n",
       "  0.5228518649188036,\n",
       "  0.5228368278689155,\n",
       "  0.5228415077976863,\n",
       "  0.5228949020077467,\n",
       "  0.5228737567198805,\n",
       "  0.5228548699041624,\n",
       "  0.5228504156592197,\n",
       "  0.5228486999450256,\n",
       "  0.5228344153260874,\n",
       "  0.5228679995864458,\n",
       "  0.5228217540953477,\n",
       "  0.5228183655580498,\n",
       "  0.5228843883989196,\n",
       "  0.5227947145658337,\n",
       "  0.5227905923737736,\n",
       "  0.5227829312148379,\n",
       "  0.5227961179633153,\n",
       "  0.5228029786034061,\n",
       "  0.5228013000826377,\n",
       "  0.5227588818163822,\n",
       "  0.5227756968457397,\n",
       "  0.5227632826574816,\n",
       "  0.5227900363452999,\n",
       "  0.5227859271384208,\n",
       "  0.5227870770186638,\n",
       "  0.5228186711814483,\n",
       "  0.5228026655877658,\n",
       "  0.5227785647896667,\n",
       "  0.5227548491386707,\n",
       "  0.5227476486072347,\n",
       "  0.5227602606307328,\n",
       "  0.5227681684261498,\n",
       "  0.5227574857155236,\n",
       "  0.5227002253933766,\n",
       "  0.5227031913533753,\n",
       "  0.5227485472112091,\n",
       "  0.5227116715796799,\n",
       "  0.5227066603265504,\n",
       "  0.5227379021895568,\n",
       "  0.5227530656277313,\n",
       "  0.5227022269606497,\n",
       "  0.5226966679303396,\n",
       "  0.5227095806354153,\n",
       "  0.5226856632963497,\n",
       "  0.5226748703107181,\n",
       "  0.5226914998115786,\n",
       "  0.5226829661374156,\n",
       "  0.5226959752975485,\n",
       "  0.522663498219098,\n",
       "  0.5226655210251219,\n",
       "  0.5226433287025911,\n",
       "  0.5226960066149731,\n",
       "  0.5226959116215171,\n",
       "  0.522641143006194,\n",
       "  0.5226501764455308,\n",
       "  0.5226425961140541,\n",
       "  0.522609852363195,\n",
       "  0.5225947886781666,\n",
       "  0.5226309245801061,\n",
       "  0.52263805845691,\n",
       "  0.5226114129700805,\n",
       "  0.522632177579883,\n",
       "  0.5226105310925613,\n",
       "  0.522607716373382,\n",
       "  0.5226240259603907,\n",
       "  0.522620859577374,\n",
       "  0.5226182047479598,\n",
       "  0.5226035353089955,\n",
       "  0.5226037381012342,\n",
       "  0.5226181401491314,\n",
       "  0.5225811714304506,\n",
       "  0.5225606029461335,\n",
       "  0.5225634387006838,\n",
       "  0.5225624187526926,\n",
       "  0.5225593517319992,\n",
       "  0.5225748663581938,\n",
       "  0.5225817694135578,\n",
       "  0.5225393440995918,\n",
       "  0.5225961981246738,\n",
       "  0.522532768672086,\n",
       "  0.5225053036830497,\n",
       "  0.5225766383774263,\n",
       "  0.522538791668964,\n",
       "  0.5225724838715332,\n",
       "  0.5225226038486677,\n",
       "  0.5225340034868419,\n",
       "  0.5224745158156981,\n",
       "  0.522494052471771,\n",
       "  0.5225613223119603,\n",
       "  0.5225521359258437,\n",
       "  0.5224774293629496,\n",
       "  0.5225343961218055,\n",
       "  0.5224969266460958,\n",
       "  0.5225127858686864,\n",
       "  0.5225133843852858,\n",
       "  0.5224958574096631,\n",
       "  0.5224761347705246,\n",
       "  0.5225011201978387,\n",
       "  0.5224696792337817,\n",
       "  0.5225040827072139,\n",
       "  0.5224921646475809,\n",
       "  0.5224573698801246,\n",
       "  0.5224696444793935,\n",
       "  0.522458124179435,\n",
       "  0.5224673866872009,\n",
       "  0.5224712488607048,\n",
       "  0.5224446867731757,\n",
       "  0.5224571134252648,\n",
       "  0.5224573603098996,\n",
       "  0.5224084780734775,\n",
       "  0.5224692468267209,\n",
       "  0.522405169424905,\n",
       "  0.5224593390956208,\n",
       "  0.5224368977484161,\n",
       "  0.5225250316138526,\n",
       "  0.5224444462054508,\n",
       "  0.5224553360824952,\n",
       "  0.5224187640914132,\n",
       "  0.5224510454670739,\n",
       "  0.5224471181217879,\n",
       "  0.5224368557125824,\n",
       "  0.5224317719857356,\n",
       "  0.5224316746769394,\n",
       "  0.522423305718254,\n",
       "  0.5224157693296743,\n",
       "  0.5224038979889181,\n",
       "  0.522384273436427,\n",
       "  0.5224119018424986,\n",
       "  0.5223907412063016,\n",
       "  0.5224405610538453,\n",
       "  0.5223752698840276,\n",
       "  0.5223886202096638,\n",
       "  0.5223682039496736,\n",
       "  0.5223355936537833,\n",
       "  0.5224029666468972,\n",
       "  0.5223668142947344,\n",
       "  0.5223436136251981,\n",
       "  0.5223384422326324,\n",
       "  0.5223450273517837,\n",
       "  0.5223180509610147,\n",
       "  0.5223495717212138,\n",
       "  0.522358751038371,\n",
       "  0.5223700699500172,\n",
       "  0.5223323533286183,\n",
       "  0.522340399697644,\n",
       "  0.5223363434134789,\n",
       "  0.522314380475071,\n",
       "  0.5223246387521795,\n",
       "  0.5223641587026467,\n",
       "  0.5223140662975867,\n",
       "  0.5223416150372844,\n",
       "  0.5222850558223319,\n",
       "  0.5223199488274526,\n",
       "  0.522321163972061,\n",
       "  0.522329047190418,\n",
       "  0.522333310460388,\n",
       "  0.5223279945286646,\n",
       "  0.522290154020245,\n",
       "  0.5222659701780009,\n",
       "  0.52231176303715,\n",
       "  0.5222691266485382,\n",
       "  0.5222941600127303,\n",
       "  0.5222801266624718,\n",
       "  0.5223264205237678,\n",
       "  0.5222874095125644,\n",
       "  0.522278270979849,\n",
       "  0.5222874769010326,\n",
       "  0.5222354311729233,\n",
       "  0.5222634916878162,\n",
       "  0.5222743594905839,\n",
       "  0.522287731222683,\n",
       "  0.5222740003851776,\n",
       "  0.5222785629403874,\n",
       "  0.5222633783067184,\n",
       "  0.5222539732192208,\n",
       "  0.5222742333405856,\n",
       "  0.5222347190498391,\n",
       "  0.5222343602563321,\n",
       "  0.5221968776064803,\n",
       "  0.5222932081392341,\n",
       "  0.5222927819420813,\n",
       "  0.5222552574021008,\n",
       "  0.5222144729533197,\n",
       "  0.522247025297404,\n",
       "  0.522275775223646,\n",
       "  0.5222273489787307,\n",
       "  0.5222149622163651,\n",
       "  0.5222363179307943,\n",
       "  0.5222466058254381,\n",
       "  0.5222488633269801,\n",
       "  0.5221846651336115,\n",
       "  0.5221893756922624,\n",
       "  0.5221842809252418,\n",
       "  0.5222238533726918,\n",
       "  0.5222081235852873,\n",
       "  0.5222349925878272,\n",
       "  0.5221755319601007,\n",
       "  0.5222145578847971,\n",
       "  0.5222190581876329,\n",
       "  0.5221674315414255,\n",
       "  0.5221688130702867,\n",
       "  0.5221795190822971,\n",
       "  0.522225838563353,\n",
       "  0.5221713492436343,\n",
       "  0.5222610027425535,\n",
       "  0.522191755027298,\n",
       "  0.5221968075998791,\n",
       "  0.522176783255435,\n",
       "  0.5221990702139898,\n",
       "  0.5221646341527694,\n",
       "  0.522161781124612,\n",
       "  0.5222206814870425,\n",
       "  0.522174645325927,\n",
       "  0.5220846060485173,\n",
       "  0.5221465417029041,\n",
       "  0.5221464070845735,\n",
       "  0.5221027222201857,\n",
       "  0.5221457888970678,\n",
       "  0.5221825593729904,\n",
       "  0.5221495163968429,\n",
       "  0.5221776995299106,\n",
       "  0.5221187216419042,\n",
       "  0.522126207054295,\n",
       "  0.5221291783074757,\n",
       "  0.5221779206247965,\n",
       "  0.522157506274905,\n",
       "  0.5221374727783785,\n",
       "  0.5221734057962549,\n",
       "  0.5220940907872111,\n",
       "  0.5221169205386285,\n",
       "  0.5221440175130172,\n",
       "  0.522159692436495,\n",
       "  0.5221145614740138,\n",
       "  0.5221606806011098,\n",
       "  0.5221217371187671,\n",
       "  0.5221007646724563,\n",
       "  0.5220937480842187,\n",
       "  0.5221211350422656,\n",
       "  0.5221253711367735,\n",
       "  0.5220704977247935,\n",
       "  0.5220920191882937,\n",
       "  0.5220790808953311,\n",
       "  0.5220884056660954,\n",
       "  0.5220806271480175,\n",
       "  0.5220899859256741,\n",
       "  0.5221004736916308,\n",
       "  0.5220774673945691,\n",
       "  0.5221037775767189,\n",
       "  0.5220963292484753,\n",
       "  0.5220847420176528,\n",
       "  0.5220652860698948,\n",
       "  0.5220195247338265,\n",
       "  0.5220755909899923,\n",
       "  0.5221229236741215,\n",
       "  0.5220932563669582,\n",
       "  0.522076429983251,\n",
       "  0.5220872971986462,\n",
       "  0.5220922223683196,\n",
       "  0.5220928635749066,\n",
       "  0.5220616652503145,\n",
       "  0.5220874888710288,\n",
       "  0.5220669805301608,\n",
       "  0.5220127331972708,\n",
       "  0.5220629967660045,\n",
       "  0.522055858202363,\n",
       "  0.522074745890792,\n",
       "  0.5220391007233645,\n",
       "  0.5220456189199995,\n",
       "  0.522061351777829,\n",
       "  0.5220679617479957,\n",
       "  0.5219928675185198,\n",
       "  0.5220373423123529,\n",
       "  0.5220348319758048,\n",
       "  0.5220082905654557,\n",
       "  0.5220411967642614,\n",
       "  0.5220309817713428,\n",
       "  0.5220516801317546,\n",
       "  0.5220614923958582,\n",
       "  0.5220299248446886,\n",
       "  0.5220194801223571,\n",
       "  0.5220522920539541,\n",
       "  0.5220518183980173,\n",
       "  0.5220075152709948,\n",
       "  0.5220080968304396,\n",
       "  0.5220290536074138,\n",
       "  0.5219968644279805,\n",
       "  0.5220542466625446,\n",
       "  0.5219721822039993,\n",
       "  0.5219815736703751,\n",
       "  0.5219969552263526,\n",
       "  0.5219888962364078,\n",
       "  0.5219887715426986,\n",
       "  0.5220320639588071,\n",
       "  0.5219635214486934,\n",
       "  0.5220119947494279,\n",
       "  0.5220094079330394,\n",
       "  0.5220172153651716,\n",
       "  0.5219460352825557,\n",
       "  0.5220006061036181,\n",
       "  0.521949622111939,\n",
       "  0.521963277770323,\n",
       "  0.5220170473758895,\n",
       "  0.5219903853879918,\n",
       "  0.5220023335478072,\n",
       "  0.5219655616725115,\n",
       "  0.5219955058005742,\n",
       "  0.5219557191549319,\n",
       "  0.5219874711585079,\n",
       "  0.5219768379116244,\n",
       "  0.5219622466933692,\n",
       "  0.5219770989129291,\n",
       "  0.5219661379456042,\n",
       "  0.5219677027635101,\n",
       "  0.5219602351741549,\n",
       "  0.5219651993957491,\n",
       "  0.5219860294619135,\n",
       "  0.5219496732323149,\n",
       "  0.5219697173971645,\n",
       "  0.5219484121474396,\n",
       "  0.5219353726010267,\n",
       "  0.5219704017262999,\n",
       "  0.521947431472962,\n",
       "  0.521979311592792,\n",
       "  0.5219701624949612,\n",
       "  0.5219505954791692,\n",
       "  0.5219467792922238,\n",
       "  0.521941587888015,\n",
       "  0.5219520473417086,\n",
       "  0.5219305638176125,\n",
       "  0.521938747636347,\n",
       "  0.5219637723652532,\n",
       "  0.5219173652002652,\n",
       "  0.5219063271672452,\n",
       "  0.5219322975658107,\n",
       "  0.5219578714947961,\n",
       "  0.5219376235056322,\n",
       "  0.5218993365387455,\n",
       "  0.5219252760046381,\n",
       "  0.5219184505219003,\n",
       "  0.5219216254278888,\n",
       "  0.5219100448463653,\n",
       "  0.5219499218312578,\n",
       "  0.5219259741423384,\n",
       "  0.5219306154024224,\n",
       "  0.5219097109380232,\n",
       "  0.5218643622796758,\n",
       "  0.5219055987677211,\n",
       "  0.5219108713515088,\n",
       "  0.5219196136529322,\n",
       "  0.5218890507090118,\n",
       "  0.5219000960955708,\n",
       "  0.5219048264791805,\n",
       "  0.5219058630435893,\n",
       "  0.5219176009468308,\n",
       "  0.5219080836157953,\n",
       "  0.5219052381984369,\n",
       "  0.5219127472939281,\n",
       "  0.521848896130681,\n",
       "  0.5219061160045695,\n",
       "  0.5219017757834952,\n",
       "  0.5218872067260288,\n",
       "  0.5219105412034128,\n",
       "  0.5218454578875579,\n",
       "  0.5218778585043811,\n",
       "  0.5218837518604169,\n",
       "  0.5218799384373093,\n",
       "  0.5219010022382506,\n",
       "  0.5218667241627676,\n",
       "  0.5218473475907328,\n",
       "  0.5218520995925328,\n",
       "  0.5218449538977038,\n",
       "  0.5218886086603914,\n",
       "  0.5218615502507231,\n",
       "  0.5218619951109907,\n",
       "  0.5218187265252052,\n",
       "  0.5218331495287912,\n",
       "  0.5218632365849112,\n",
       "  0.5218150610888433,\n",
       "  0.5217774453793989,\n",
       "  0.5218445100634408,\n",
       "  0.5218378810051869,\n",
       "  0.5218799108087963,\n",
       "  0.5218489572356299,\n",
       "  0.521832718976431,\n",
       "  0.5218031680456614,\n",
       "  0.5218274249285055,\n",
       "  0.5218873720432975,\n",
       "  0.5218560806679802,\n",
       "  0.5218375043947153,\n",
       "  0.5218427573182209,\n",
       "  0.5218175299359508,\n",
       "  0.5217969667319149,\n",
       "  0.521849227219787,\n",
       "  0.5217891519174063,\n",
       "  0.5217817348360115,\n",
       "  0.5218424565842807,\n",
       "  0.5218048683675098,\n",
       "  0.5218364103801195,\n",
       "  0.5218608627790691,\n",
       "  0.5218334749832195,\n",
       "  0.5217789267797912,\n",
       "  0.5218380993961864,\n",
       "  0.5218100717885699,\n",
       "  0.521796736405264,\n",
       "  0.5218292726315141,\n",
       "  0.5217900767952947,\n",
       "  0.5218188040204259,\n",
       "  0.5218062839503139,\n",
       "  0.5218220442105106,\n",
       "  0.5217759694839157,\n",
       "  0.52180686307148,\n",
       "  0.521818774827104,\n",
       "  0.521835199241634,\n",
       "  0.5218147908572915,\n",
       "  0.5218060288432235,\n",
       "  0.5217982806590629,\n",
       "  0.5217920764630933,\n",
       "  0.5217735054002927,\n",
       "  0.5217888735279037,\n",
       "  0.5218396555788065,\n",
       "  0.5217186494971725,\n",
       "  0.5217863354801247,\n",
       "  0.5218061721234007,\n",
       "  0.5218055139786313,\n",
       "  0.5217655395255779,\n",
       "  0.5217599812860199,\n",
       "  0.5217568340593894,\n",
       "  0.5217900724491937,\n",
       "  0.5217597716570389,\n",
       "  0.5217598221179489,\n",
       "  0.5217701557540333,\n",
       "  0.5217884464914306,\n",
       "  0.5217761446134918,\n",
       "  0.5217808166005924,\n",
       "  0.5217465779359799,\n",
       "  0.5218000687650155,\n",
       "  0.5217513198264742,\n",
       "  0.5217776768504393,\n",
       "  0.5217393754405647,\n",
       "  0.5217792792480406,\n",
       "  0.5217448027332495,\n",
       "  0.5217601627279527,\n",
       "  0.521776290869993,\n",
       "  0.5217181083389356,\n",
       "  0.5217511073554908,\n",
       "  0.5217560711149277,\n",
       "  0.5217618391625622,\n",
       "  0.5217609835293605,\n",
       "  0.5217489441674492,\n",
       "  0.521715515055379,\n",
       "  0.5217628251370517,\n",
       "  0.5217775809782013,\n",
       "  0.5216891198430975,\n",
       "  0.5217345315665787,\n",
       "  0.5217311071927307,\n",
       "  0.5217646131086827,\n",
       "  0.5217120632199715,\n",
       "  0.521741783887722,\n",
       "  0.5217205022907423,\n",
       "  0.5217191819389234,\n",
       "  0.5217229902244191,\n",
       "  0.5217145638959881,\n",
       "  0.5216921813512928,\n",
       "  0.5217416114256062,\n",
       "  0.5217166573210287,\n",
       "  0.5216624630580752,\n",
       "  0.5217142891870494,\n",
       "  0.521750772786165,\n",
       "  0.521710312224727,\n",
       "  0.5216942580878221,\n",
       "  0.5216682370082717,\n",
       "  0.5216912055374362,\n",
       "  0.521716935366759,\n",
       "  0.5217029472479744,\n",
       "  0.5216722774090964,\n",
       "  0.5217147648509805,\n",
       "  0.5216384492282851,\n",
       "  0.5217050072064446,\n",
       "  0.5217205800766137,\n",
       "  0.5216933185696373,\n",
       "  0.5216865851928204,\n",
       "  0.5216798438333539,\n",
       "  0.5217085248093544,\n",
       "  0.5216527897766163,\n",
       "  0.5216905491745151,\n",
       "  0.5216675725139606,\n",
       "  0.5217187305963101,\n",
       "  0.5216981798940493],\n",
       " 'val_loss': [0.5186804910985435,\n",
       "  0.5188068808094273,\n",
       "  0.5188186939169721,\n",
       "  0.5185571073032007,\n",
       "  0.5185590442118606,\n",
       "  0.5186228853900258,\n",
       "  0.5187436642685557,\n",
       "  0.518351762517681,\n",
       "  0.5186623153163166,\n",
       "  0.5182918407083527,\n",
       "  0.5185011395109378,\n",
       "  0.5187720428637372,\n",
       "  0.5185062441884017,\n",
       "  0.5186271858893758,\n",
       "  0.5187321758367182,\n",
       "  0.518756133027193,\n",
       "  0.5186880100064162,\n",
       "  0.5183158822175933,\n",
       "  0.5183615725699479,\n",
       "  0.5184280315065771,\n",
       "  0.5186766584714254,\n",
       "  0.5184459327682247,\n",
       "  0.5185381610219072,\n",
       "  0.5185721111976034,\n",
       "  0.5185124338642368,\n",
       "  0.5183843325793258,\n",
       "  0.5186909979921046,\n",
       "  0.5188451586215477,\n",
       "  0.518293573846662,\n",
       "  0.5186379728763084,\n",
       "  0.5184437535642609,\n",
       "  0.5185966690381368,\n",
       "  0.5184922891903699,\n",
       "  0.518463695921549,\n",
       "  0.5184119840463003,\n",
       "  0.518501940781508,\n",
       "  0.5183655617198324,\n",
       "  0.5184778655932202,\n",
       "  0.5185636611488776,\n",
       "  0.5184256058882891,\n",
       "  0.5186293161496883,\n",
       "  0.518627801319448,\n",
       "  0.5182053122094007,\n",
       "  0.518611402288685,\n",
       "  0.5182886688205285,\n",
       "  0.5184479096556097,\n",
       "  0.5185223640949745,\n",
       "  0.5184752756987161,\n",
       "  0.5181903950567168,\n",
       "  0.5183477767599307,\n",
       "  0.5183833934427277,\n",
       "  0.5184105915751883,\n",
       "  0.5183065683861089,\n",
       "  0.5183180602100806,\n",
       "  0.5186133835373855,\n",
       "  0.5183842596484394,\n",
       "  0.5184433460235596,\n",
       "  0.518635938322641,\n",
       "  0.5185645205703208,\n",
       "  0.5183502186120041,\n",
       "  0.5183935897137092,\n",
       "  0.5184651416976277,\n",
       "  0.5186417144003922,\n",
       "  0.5185180161057449,\n",
       "  0.5185954565924358,\n",
       "  0.5184569891875352,\n",
       "  0.5182665009808735,\n",
       "  0.5185965827810086,\n",
       "  0.518673460900299,\n",
       "  0.5186894153191791,\n",
       "  0.5186361529962803,\n",
       "  0.5182374541352435,\n",
       "  0.5181928865793275,\n",
       "  0.5183366665510627,\n",
       "  0.5182341799988011,\n",
       "  0.5182293631681582,\n",
       "  0.5182090481606926,\n",
       "  0.518315524105134,\n",
       "  0.5181022310644631,\n",
       "  0.5183695634690727,\n",
       "  0.5184200788901104,\n",
       "  0.5183767693314126,\n",
       "  0.5183665391390886,\n",
       "  0.5180449282250753,\n",
       "  0.5183093930647625,\n",
       "  0.5185933709144592,\n",
       "  0.5182746484027645,\n",
       "  0.5185033768657746,\n",
       "  0.5182469557455884,\n",
       "  0.5183241774396199,\n",
       "  0.5181717325032242,\n",
       "  0.5181520336527166,\n",
       "  0.5184206024902623,\n",
       "  0.5182432125254375,\n",
       "  0.518351501081048,\n",
       "  0.5180573950453502,\n",
       "  0.5183165793011828,\n",
       "  0.5184108084294854,\n",
       "  0.5181763298143216,\n",
       "  0.5179996790924692,\n",
       "  0.5180710830339571,\n",
       "  0.5182701499481511,\n",
       "  0.5184292061542107,\n",
       "  0.5181615609463638,\n",
       "  0.5180207452153772,\n",
       "  0.5182288938421544,\n",
       "  0.5181962302545222,\n",
       "  0.5183449004723774,\n",
       "  0.5182534543479361,\n",
       "  0.5181778763852468,\n",
       "  0.5184400028329554,\n",
       "  0.5185513653890873,\n",
       "  0.5184969671858035,\n",
       "  0.5182289311556312,\n",
       "  0.5186571757483288,\n",
       "  0.5182202833939374,\n",
       "  0.5182930507311007,\n",
       "  0.5184481425013968,\n",
       "  0.5182243689773528,\n",
       "  0.518216224947596,\n",
       "  0.518079400062561,\n",
       "  0.5183684838011982,\n",
       "  0.5180630666938254,\n",
       "  0.5182806578108935,\n",
       "  0.5178627330597824,\n",
       "  0.5180402904506621,\n",
       "  0.5180411804013136,\n",
       "  0.5182556674732426,\n",
       "  0.5178056276910673,\n",
       "  0.5179903807678843,\n",
       "  0.5180361280596353,\n",
       "  0.517920322049924,\n",
       "  0.5181775824810432,\n",
       "  0.517993994602343,\n",
       "  0.5180491931070157,\n",
       "  0.5179805137762209,\n",
       "  0.5181352956508233,\n",
       "  0.5177787993012405,\n",
       "  0.5181708776853918,\n",
       "  0.5182425316756334,\n",
       "  0.5180163897149931,\n",
       "  0.5181551590198423,\n",
       "  0.5182947305644431,\n",
       "  0.5183991181656598,\n",
       "  0.5180546810956506,\n",
       "  0.5183208521788683,\n",
       "  0.5179208403195792,\n",
       "  0.5184319111389842,\n",
       "  0.5179477728963867,\n",
       "  0.5180970500639783,\n",
       "  0.5182476843275675,\n",
       "  0.5180066523513174,\n",
       "  0.5180603214880315,\n",
       "  0.5180883489973177,\n",
       "  0.5184218670294537,\n",
       "  0.5181733180836934,\n",
       "  0.5180940233110413,\n",
       "  0.5179164402368592,\n",
       "  0.5180493859740777,\n",
       "  0.5178522838325035,\n",
       "  0.5181336802680317,\n",
       "  0.5180107989931494,\n",
       "  0.5181512006414615,\n",
       "  0.5180212520971531,\n",
       "  0.5181931206365911,\n",
       "  0.517833401274875,\n",
       "  0.5180970919810659,\n",
       "  0.5180489367585841,\n",
       "  0.517722800737474,\n",
       "  0.517958166638041,\n",
       "  0.5180344957161725,\n",
       "  0.5178516868168745,\n",
       "  0.5182735445053597,\n",
       "  0.5180544732062797,\n",
       "  0.5179879556341869,\n",
       "  0.517984918704847,\n",
       "  0.5179042602942242,\n",
       "  0.5179663768144158,\n",
       "  0.5179412532143477,\n",
       "  0.5179890193105713,\n",
       "  0.5181717984075469,\n",
       "  0.5179243635355941,\n",
       "  0.517945314810528,\n",
       "  0.5177534615121236,\n",
       "  0.5181056898299271,\n",
       "  0.5179741624894181,\n",
       "  0.5179454587339386,\n",
       "  0.5179578935712333,\n",
       "  0.5181636131875883,\n",
       "  0.5181171787463552,\n",
       "  0.5179360378079298,\n",
       "  0.5178943404821845,\n",
       "  0.5180596479070865,\n",
       "  0.5180633048701092,\n",
       "  0.5177323738249336,\n",
       "  0.5182438725378455,\n",
       "  0.5178842001814183,\n",
       "  0.5178781360145507,\n",
       "  0.5182327998847496,\n",
       "  0.5178779879721199,\n",
       "  0.517808861364194,\n",
       "  0.5180062438414349,\n",
       "  0.5179407259797663,\n",
       "  0.5178880042176905,\n",
       "  0.5181027381885343,\n",
       "  0.5180243881252723,\n",
       "  0.5177836527184743,\n",
       "  0.5178626797548155,\n",
       "  0.517461579505021,\n",
       "  0.517826170698414,\n",
       "  0.5178217359674655,\n",
       "  0.5179850226495324,\n",
       "  0.5181126657540236,\n",
       "  0.5179565766962563,\n",
       "  0.5177802489540442,\n",
       "  0.5177406821309066,\n",
       "  0.5180638754755501,\n",
       "  0.5179541086762901,\n",
       "  0.5176581015916375,\n",
       "  0.5177747207443889,\n",
       "  0.5179833571116129,\n",
       "  0.5182058338711901,\n",
       "  0.5179811597355013,\n",
       "  0.5180563211925631,\n",
       "  0.5177402304924601,\n",
       "  0.5177931596593159,\n",
       "  0.5179502590885007,\n",
       "  0.5179074736145454,\n",
       "  0.5177676234303451,\n",
       "  0.5180409414981438,\n",
       "  0.517769730914899,\n",
       "  0.5176561179199839,\n",
       "  0.5177668715880169,\n",
       "  0.5179711611774879,\n",
       "  0.5178044828457561,\n",
       "  0.51806629067514,\n",
       "  0.5181027931895682,\n",
       "  0.5177799213707932,\n",
       "  0.5178444795007628,\n",
       "  0.5178362460640388,\n",
       "  0.5175990784555916,\n",
       "  0.5179125106916195,\n",
       "  0.5177652125920706,\n",
       "  0.5177047184812344,\n",
       "  0.5177399559718806,\n",
       "  0.517909318693285,\n",
       "  0.5181687139883274,\n",
       "  0.5177913121576232,\n",
       "  0.5181765125049809,\n",
       "  0.5178492139510023,\n",
       "  0.5180462109364146,\n",
       "  0.5179894069830576,\n",
       "  0.5177922498404495,\n",
       "  0.5179703950397367,\n",
       "  0.5180512245108442,\n",
       "  0.5178891699003979,\n",
       "  0.517854461098105,\n",
       "  0.517748179232202,\n",
       "  0.5179126827212853,\n",
       "  0.5178696808776235,\n",
       "  0.5177819401752658,\n",
       "  0.5176659085886265,\n",
       "  0.5180255722224227,\n",
       "  0.5179034066878683,\n",
       "  0.5175690430451215,\n",
       "  0.517828109303141,\n",
       "  0.518113162701692,\n",
       "  0.5178465688131689,\n",
       "  0.5178134174850898,\n",
       "  0.5179522992149601,\n",
       "  0.5175543914480907,\n",
       "  0.5177029618402806,\n",
       "  0.5179406050744095,\n",
       "  0.5179942916563856,\n",
       "  0.517860117481976,\n",
       "  0.5180319843253469,\n",
       "  0.5175937106454276,\n",
       "  0.5179621066019787,\n",
       "  0.5177208156120486,\n",
       "  0.5176440676053365,\n",
       "  0.5179376163618351,\n",
       "  0.5179155875996846,\n",
       "  0.5177064392624832,\n",
       "  0.5175554006080317,\n",
       "  0.5178904184480992,\n",
       "  0.5180978685375152,\n",
       "  0.5178067475799623,\n",
       "  0.5179430217277713,\n",
       "  0.5181326199837817,\n",
       "  0.5179070251259378,\n",
       "  0.517774213862613,\n",
       "  0.5176672625347851,\n",
       "  0.517911910041561,\n",
       "  0.5179307397788133,\n",
       "  0.5177936418269707,\n",
       "  0.5176444828994875,\n",
       "  0.5177185843145944,\n",
       "  0.5175700965451031,\n",
       "  0.5178898192518125,\n",
       "  0.5176151598372111,\n",
       "  0.517690140541976,\n",
       "  0.5179095636538373,\n",
       "  0.5178323317834033,\n",
       "  0.5176330678831271,\n",
       "  0.5178362230459849,\n",
       "  0.5176659015620627,\n",
       "  0.5177418138922715,\n",
       "  0.5176993058464392,\n",
       "  0.5178038812265163,\n",
       "  0.5181101768966613,\n",
       "  0.5179183764186331,\n",
       "  0.5176045269500918,\n",
       "  0.5178309722644526,\n",
       "  0.5176243484020233,\n",
       "  0.5174951890135199,\n",
       "  0.5175453375994674,\n",
       "  0.5178737046757365,\n",
       "  0.5177580990442415,\n",
       "  0.5177547439811675,\n",
       "  0.5176869645351316,\n",
       "  0.5177662515543341,\n",
       "  0.5177776292572177,\n",
       "  0.5181579473542004,\n",
       "  0.5179208744832171,\n",
       "  0.5175391646420083,\n",
       "  0.5177179618579585,\n",
       "  0.5178860748201851,\n",
       "  0.5177145854728978,\n",
       "  0.5177991901471363,\n",
       "  0.5177128663877162,\n",
       "  0.5179206421220206,\n",
       "  0.51783796878365,\n",
       "  0.5179233463799081,\n",
       "  0.5181165199454237,\n",
       "  0.517666386394966,\n",
       "  0.5178273196627454,\n",
       "  0.5178948078698259,\n",
       "  0.5177263343721871,\n",
       "  0.5178170361654545,\n",
       "  0.5173890023696713,\n",
       "  0.5179579829782005,\n",
       "  0.517949706897503,\n",
       "  0.5178519005213327,\n",
       "  0.5177850493086062,\n",
       "  0.5175931642695171,\n",
       "  0.5178380623096372,\n",
       "  0.5175621325407571,\n",
       "  0.517540530702932,\n",
       "  0.5175878943466559,\n",
       "  0.5177168635333457,\n",
       "  0.5180060434632185,\n",
       "  0.5177051759347683,\n",
       "  0.5180117868311037,\n",
       "  0.5175874855944781,\n",
       "  0.5177236509516956,\n",
       "  0.5177125085175522,\n",
       "  0.5179060261423994,\n",
       "  0.5176071728148112,\n",
       "  0.5177989708698862,\n",
       "  0.5177490323539672,\n",
       "  0.5178371255959922,\n",
       "  0.517781952290031,\n",
       "  0.5173117586267673,\n",
       "  0.5177643667391645,\n",
       "  0.5175156084502616,\n",
       "  0.5176905727967983,\n",
       "  0.5174920871490385,\n",
       "  0.5177659276055127,\n",
       "  0.5176497714790872,\n",
       "  0.5173663019649382,\n",
       "  0.5175782904876927,\n",
       "  0.5176406980045443,\n",
       "  0.517852293766611,\n",
       "  0.5176191988999281,\n",
       "  0.5175788768423282,\n",
       "  0.517631985307709,\n",
       "  0.5174944148800238,\n",
       "  0.5176708504436461,\n",
       "  0.5176427776251382,\n",
       "  0.5174018171260027,\n",
       "  0.5178141492169078,\n",
       "  0.517436237596884,\n",
       "  0.5175734782606606,\n",
       "  0.5176707089431887,\n",
       "  0.5175397803143758,\n",
       "  0.5177246923369121,\n",
       "  0.5176411256557558,\n",
       "  0.517700294168984,\n",
       "  0.5176505727496573,\n",
       "  0.5174636506452793,\n",
       "  0.5174030680966571,\n",
       "  0.5176231005812079,\n",
       "  0.5176196439964015,\n",
       "  0.5177363166964151,\n",
       "  0.5174302521759901,\n",
       "  0.5179400996464055,\n",
       "  0.5176062983710591,\n",
       "  0.5175329776314216,\n",
       "  0.5173560041722244,\n",
       "  0.5176120349546758,\n",
       "  0.517585295002635,\n",
       "  0.5174841907450823,\n",
       "  0.5174976262619825,\n",
       "  0.5173973283632015,\n",
       "  0.5173821938716299,\n",
       "  0.517416616523169,\n",
       "  0.5174436561945008,\n",
       "  0.5177500197073308,\n",
       "  0.5177188193410392,\n",
       "  0.5176844153462387,\n",
       "  0.5173814388794628,\n",
       "  0.5173278780487495,\n",
       "  0.5174912136744677,\n",
       "  0.5173726658510968,\n",
       "  0.5175376977862381,\n",
       "  0.517424354223701,\n",
       "  0.5175218688763255,\n",
       "  0.5174877163840503,\n",
       "  0.5173907866322898,\n",
       "  0.5176211905673267,\n",
       "  0.5173315679639335,\n",
       "  0.5176632227451821,\n",
       "  0.517324141612867,\n",
       "  0.5172875099550418,\n",
       "  0.5174365324702689,\n",
       "  0.5175685974640575,\n",
       "  0.5175330999905501,\n",
       "  0.517448907702919,\n",
       "  0.5174408528378339,\n",
       "  0.5172263962951132,\n",
       "  0.5177114060739192,\n",
       "  0.5174184562714119,\n",
       "  0.517314384138681,\n",
       "  0.5175699039203364,\n",
       "  0.5176997519120937,\n",
       "  0.5175997215073284,\n",
       "  0.5175348915220276,\n",
       "  0.5174451877431172,\n",
       "  0.5173729248647767,\n",
       "  0.5172652229060971,\n",
       "  0.5177352018957215,\n",
       "  0.5177750376666465,\n",
       "  0.5175298631675844,\n",
       "  0.5174789758232551,\n",
       "  0.5175311994261858,\n",
       "  0.5175711323575276,\n",
       "  0.5175582601772091,\n",
       "  0.5176293009180364,\n",
       "  0.5173599334751687,\n",
       "  0.5173196177172467,\n",
       "  0.5174211581063465,\n",
       "  0.5175570060567158,\n",
       "  0.5174548475722957,\n",
       "  0.5175826094014858,\n",
       "  0.5175228707674073,\n",
       "  0.5175358238743573,\n",
       "  0.5174006911797252,\n",
       "  0.5177102464485944,\n",
       "  0.5176927149295807,\n",
       "  0.5173512198091522,\n",
       "  0.5173797721300668,\n",
       "  0.5174776066124924,\n",
       "  0.5173496783264284,\n",
       "  0.5174988372539117,\n",
       "  0.517710906945593,\n",
       "  0.5175524998486527,\n",
       "  0.5173388264043545,\n",
       "  0.5177245455059579,\n",
       "  0.5175277445374465,\n",
       "  0.5174283433735856,\n",
       "  0.5172219012326341,\n",
       "  0.5173874943236995,\n",
       "  0.5174193256269626,\n",
       "  0.5175268027356,\n",
       "  0.5172535481491709,\n",
       "  0.5175570765646492,\n",
       "  0.5176542079061027,\n",
       "  0.517431504358121,\n",
       "  0.5173979677805086,\n",
       "  0.5172627764504131,\n",
       "  0.5173739073722343,\n",
       "  0.5176398703237859,\n",
       "  0.5173936762460848,\n",
       "  0.517549570740723,\n",
       "  0.5175253654398569,\n",
       "  0.5176161583361587,\n",
       "  0.5172827374644395,\n",
       "  0.5174451901660702,\n",
       "  0.5173667017521897,\n",
       "  0.5173253688385816,\n",
       "  0.5175756342042752,\n",
       "  0.5173894680612455,\n",
       "  0.5173872055076971,\n",
       "  0.5175172117182879,\n",
       "  0.5174213684186703,\n",
       "  0.517533810158086,\n",
       "  0.5174939969206244,\n",
       "  0.517479509842105,\n",
       "  0.5174325362938207,\n",
       "  0.5176390244708797]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_callback.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_directory = base_dir + '/tiny_test16/class0'\n",
    "files = os.listdir(images_directory)\n",
    "files.sort()\n",
    "\n",
    "images = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for file in files:\n",
    "    counter += 1\n",
    "    if counter > 100:\n",
    "        break\n",
    "    if not file.startswith('.'):\n",
    "        img = imageio.imread(images_directory + '/' + file)\n",
    "        img = img[4: 12, 4: 12, :]\n",
    "        img = np.expand_dims(img, axis=-1)        \n",
    "        images.append(img)\n",
    "\n",
    "\n",
    "images = np.array(images).reshape(np.array(images).shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "images = images / 255\n",
    "predictions = autoencoder.predict_on_batch(np.array(images))\n",
    "print(\"predictions: \")\n",
    "for i, im1 in enumerate(images):\n",
    "    im_1 = im1.reshape(input_shape)\n",
    "    plt.imshow(im_1, interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "    pred_1 = predictions[i].numpy()#.reshape(input_shape)\n",
    "    plt.imshow(pred_1, interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "    if i == 2:\n",
    "        break\n",
    "    print(\"next\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f4b48aa48d0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f4b4dd34d68>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f4b48a0eb70>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f4b48a0e1d0>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f4b489dd940>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f4b489ddcc0>\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 8, 8, 3)]         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 8, 8, 32)          896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 32)          0         \n",
      "=================================================================\n",
      "Total params: 19,392\n",
      "Trainable params: 19,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = Model(input_img, encoded)\n",
    "for i in range(1, len(encoder.layers)):\n",
    "    print(encoder.get_layer(index=i))\n",
    "    encoder.get_layer(index=i).set_weights(autoencoder.get_layer(index=i).get_weights())\n",
    "encoder.summary()\n",
    "\n",
    "# encoder.save(base_dir + '/encoder' + model_version + '.h5')\n",
    "encoder.save(base_dir + '/' + model_version + '__encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = encoder.predict_on_batch(np.array(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 32), dtype=float32, numpy=\n",
       "array([[[ 0.15799563,  0.16049819,  0.21916685,  0.21493381,\n",
       "          0.20733698,  0.03671741,  0.05747575,  0.10735112,\n",
       "          0.06713152,  0.02541121,  0.05364724,  0.28972316,\n",
       "          0.04158654,  0.18401614,  0.21080507,  0.08417755,\n",
       "          0.1331399 ,  0.16538614, -0.05397493,  0.1684546 ,\n",
       "          0.02602411,  0.23868361,  0.14836918,  0.2601838 ,\n",
       "          0.19652097,  0.22009982, -0.0424118 , -0.03687626,\n",
       "          0.22905456,  0.00349838,  0.19179514,  0.1691366 ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.29983902"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(predictions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0986481"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(predictions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 1, 1, 32])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 60081<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.90MB of 0.90MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>wandb/run-20201105_101337-2olygd6h/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>wandb/run-20201105_101337-2olygd6h/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>499</td></tr><tr><td>loss</td><td>0.5217</td></tr><tr><td>val_loss</td><td>0.51764</td></tr><tr><td>_step</td><td>499</td></tr><tr><td>_runtime</td><td>22990</td></tr><tr><td>_timestamp</td><td>1604590607</td></tr><tr><td>best_val_loss</td><td>0.51722</td></tr><tr><td>best_epoch</td><td>470</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>██▇▇▇▇▆▆▆▆▅▅▅▅▄▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>val_loss</td><td>▇██▆▆▆▆▅▅▅▅▅▅▄▄▄▄▃▂▃▄▃▂▂▃▃▃▃▃▂▂▄▃▂▂▂▁▁▂▂</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1501 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">fresh-vortex-8</strong>: <a href=\"https://wandb.ai/nimpy/patch-desc-ae/runs/2olygd6h\" target=\"_blank\">https://wandb.ai/nimpy/patch-desc-ae/runs/2olygd6h</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 46., 128., 878., 748., 512., 398., 266., 138.,  55.,  31.]),\n",
       " array([-0.29983902, -0.15999031, -0.0201416 ,  0.11970711,  0.25955582,\n",
       "         0.39940453,  0.53925323,  0.67910194,  0.81895065,  0.95879936,\n",
       "         1.0986481 ], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(predictions.numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "470"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(history_callback.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb run fresh-vortex-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
