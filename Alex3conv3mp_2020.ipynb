{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os import listdir\n",
    "from os import system\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import imageio\n",
    "from skimage.measure import block_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 8\n",
    "nb_channels = 3\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/niaki/Code/ImageNet/tiny-imagenet-200'\n",
    "\n",
    "train_data_dir      = base_dir + '/tiny_train16'\n",
    "validation_data_dir = base_dir + '/tiny_validation16'\n",
    "test_data_dir       = base_dir + '/tiny_test16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data_cropped(dir_patches):\n",
    "    \"\"\"Load all the patches from dir_patches into tensors for training the autoencoder.\n",
    "    Return:\n",
    "        patches_in  -- tensor of stacked patches centre-cropped to 8x8    \n",
    "    \"\"\"\n",
    "    files_patches = listdir(dir_patches + '/class0')\n",
    "    files_patches.sort()\n",
    "    \n",
    "    patches_in = []\n",
    "    patches_out = []\n",
    "\n",
    "    \n",
    "    for file_patch in files_patches:\n",
    "        patch_in = imageio.imread(dir_patches + '/class0/' + file_patch)\n",
    "        patch_in = patch_in[4: 12, 4: 12, :]  # center-crop to 8x8\n",
    "#         patch_out = block_reduce(patch_in, (2, 2, 1), func=np.mean)  # downsample (mean-pool)\n",
    "        \n",
    "        patches_in.append(patch_in)\n",
    "#         patches_out.append(patch_out)\n",
    "        \n",
    "\n",
    "    patches_in = np.array(patches_in)\n",
    "    patches_in = patches_in.astype(np.float64) / 255\n",
    "#     patches_in = np.expand_dims(patches_in, -1)  # need this if grayscale\n",
    "    \n",
    "#     patches_out = np.array(patches_out)\n",
    "#     patches_out = patches_out.astype(np.float64) / 255\n",
    "#     patches_out = np.expand_dims(patches_out, -1)  # need this if grayscale\n",
    "        \n",
    "    print(\"in\", patches_in.shape)# , \"; out\", patches_out.shape)\n",
    "    \n",
    "    return patches_in#, patches_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in (157086, 8, 8, 3)\n",
      "in (3932, 8, 8, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = loading_data_cropped(train_data_dir)  # y_train\n",
    "x_validation = loading_data_cropped(validation_data_dir)  # y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loading_data(dir_patches):\n",
    "#     \"\"\"Load all the patches from dir_patches into tensors for training the autoencoder.\n",
    "#     Return:\n",
    "#         patches_in  -- tensor of stacked patches in their original shape, 16x16\n",
    "#         patches_out -- tensor of the original patches downsampled to 8x8\n",
    "    \n",
    "#     \"\"\"\n",
    "#     files_patches = listdir(dir_patches + '/class0')\n",
    "#     files_patches.sort()\n",
    "    \n",
    "#     patches_in = []\n",
    "#     patches_out = []\n",
    "\n",
    "    \n",
    "#     for file_patch in files_patches:\n",
    "#         patch_in = imageio.imread(dir_patches + '/class0/' + file_patch)\n",
    "        \n",
    "#         patch_out = block_reduce(patch_in, (2, 2, 1), func=np.mean)  # downsample (mean-pool)\n",
    "        \n",
    "#         patches_in.append(patch_in)\n",
    "#         patches_out.append(patch_out)\n",
    "        \n",
    "\n",
    "#     patches_in = np.array(patches_in)\n",
    "#     patches_in = patches_in.astype(np.float64) / 255\n",
    "# #     patches_in = np.expand_dims(patches_in, -1)  # need this if grayscale\n",
    "    \n",
    "#     patches_out = np.array(patches_out)\n",
    "#     patches_out = patches_out.astype(np.float64) / 255\n",
    "# #     patches_out = np.expand_dims(patches_out, -1)  # need this if grayscale\n",
    "        \n",
    "#     print(\"in\", patches_in.shape, \"; out\", patches_out.shape)\n",
    "    \n",
    "#     return patches_in, patches_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, _ = loading_data(train_data_dir)  # y_train\n",
    "# x_validation, _ = loading_data(validation_data_dir)  # y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = x_train\n",
    "y_validation = x_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_index = np.random.randint(x_train.shape[0]) #  5429\n",
    "# print(np.array(np.round(x_train[temp_index] * 255), dtype=np.uint8))\n",
    "plt.imshow(np.array(np.round(x_train[temp_index] * 255), dtype=np.uint8))\n",
    "plt.show()\n",
    "plt.imshow(np.array(np.round(y_train[temp_index] * 255), dtype=np.uint8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 8, 8, 3)]         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 8, 8, 32)          896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 1, 32)          9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 2, 32)          9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 32)          9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 3)           867       \n",
      "=================================================================\n",
      "Total params: 48,003\n",
      "Trainable params: 48,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (patch_size, patch_size, nb_channels)\n",
    "input_img = Input(shape=input_shape)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation=\"elu\", padding=\"same\")(input_img)\n",
    "x = Conv2D(32, (3, 3), activation=\"elu\", padding=\"same\")(x)\n",
    "x = Conv2D(32, (3, 3), activation=\"elu\", padding=\"same\")(x)\n",
    "x = MaxPool2D((2, 2), padding=\"same\")(x)\n",
    "x = MaxPool2D((2, 2), padding=\"same\")(x)\n",
    "encoded = MaxPool2D((2, 2), padding=\"same\")(x)\n",
    "\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation=\"elu\", padding=\"same\")(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation=\"elu\", padding=\"same\")(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation=\"elu\", padding=\"same\")(x)  # decoded = Conv2D(3, (3, 3), activation=\"elu\", padding=\"same\")(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(3, (3, 3), activation=\"elu\", padding=\"same\")(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7fdb18c75208>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fdb18b1db70>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fdb118ee1d0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fdb118ee860>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fdb118eea90>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fdb118eebe0>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fdb118eed30>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fdb118eee80>\n",
      "<tensorflow.python.keras.layers.convolutional.UpSampling2D object at 0x7fdb118f10b8>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fdb118f11d0>\n",
      "<tensorflow.python.keras.layers.convolutional.UpSampling2D object at 0x7fdb1922fe80>\n"
     ]
    }
   ],
   "source": [
    "# model_version_dwnsmpld_output = 'patch_desc_ae_20201026_14251116_alex_3conv3mp_2020_augm_elu_lastelu_dwnsmpl'\n",
    "# autoencoder_dwnsmpld_output = load_model(base_dir + '/' + model_version_dwnsmpld_output + '.h5')\n",
    "\n",
    "# for i in range(11):\n",
    "#     print(autoencoder_dwnsmpld_output.get_layer(index=i))\n",
    "#     autoencoder.get_layer(index=i).set_weights(autoencoder_dwnsmpld_output.get_layer(index=i).get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datagen = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
    "    width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
    "    horizontal_flip=False, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "# os.environ['WANDB_MODE'] = 'dryrun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: nimpy (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.10.8 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.7<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">dreadful-bones-7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/nimpy/patch-desc-ae\" target=\"_blank\">https://wandb.ai/nimpy/patch-desc-ae</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/nimpy/patch-desc-ae/runs/kfpgf6u4\" target=\"_blank\">https://wandb.ai/nimpy/patch-desc-ae/runs/kfpgf6u4</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20201030_140320-kfpgf6u4</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "  project=\"patch-desc-ae\",\n",
    "  config={\n",
    "    \"augmentation\": True,\n",
    "    \"elus\": False,\n",
    "    \"last_layer_activation\": \"elu\",\n",
    "    \"downsampling_output\": False,\n",
    "    \"optimizer\": \"adadelta\", \n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"epochs\": 500,\n",
    "    \"patch_size\": 8}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/nimpy/patch-desc-ae/runs/kfpgf6u4?jupyter=true\" style=\"border:none;width:100%;height:420px\">\n",
       "                </iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.Run at 0x7f4b91288b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/niaki/Code/ImageNet/tiny-imagenet-200/weights_patch_desc_ae_20201030_1404318_alex_3conv3mp_2020_augm_elu_lastelu_NOTdwnsmpl\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 4909 steps, validate for 123 steps\n",
      "Epoch 1/500\n",
      "4909/4909 [==============================] - 47s 10ms/step - loss: 1.0684 - val_loss: 0.6186\n",
      "Epoch 2/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5947 - val_loss: 0.5756\n",
      "Epoch 3/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5748 - val_loss: 0.5660\n",
      "Epoch 4/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5683 - val_loss: 0.5612\n",
      "Epoch 5/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5641 - val_loss: 0.5574\n",
      "Epoch 6/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5602 - val_loss: 0.5535\n",
      "Epoch 7/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5563 - val_loss: 0.5492\n",
      "Epoch 8/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5525 - val_loss: 0.5458\n",
      "Epoch 9/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5498 - val_loss: 0.5435\n",
      "Epoch 10/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5479 - val_loss: 0.5417\n",
      "Epoch 11/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5464 - val_loss: 0.5406\n",
      "Epoch 12/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5454 - val_loss: 0.5397\n",
      "Epoch 13/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5443 - val_loss: 0.5388\n",
      "Epoch 14/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5432 - val_loss: 0.5377\n",
      "Epoch 15/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5423 - val_loss: 0.5371\n",
      "Epoch 16/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5414 - val_loss: 0.5359\n",
      "Epoch 17/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5405 - val_loss: 0.5353\n",
      "Epoch 18/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5398 - val_loss: 0.5346\n",
      "Epoch 19/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5391 - val_loss: 0.5340\n",
      "Epoch 20/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5385 - val_loss: 0.5337\n",
      "Epoch 21/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5381 - val_loss: 0.5331\n",
      "Epoch 22/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5376 - val_loss: 0.5329\n",
      "Epoch 23/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5372 - val_loss: 0.5322\n",
      "Epoch 24/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5368 - val_loss: 0.5319\n",
      "Epoch 25/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5364 - val_loss: 0.5315\n",
      "Epoch 26/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5361 - val_loss: 0.5312\n",
      "Epoch 27/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5357 - val_loss: 0.5308\n",
      "Epoch 28/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5355 - val_loss: 0.5308\n",
      "Epoch 29/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5352 - val_loss: 0.5305\n",
      "Epoch 30/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5349 - val_loss: 0.5301\n",
      "Epoch 31/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5346 - val_loss: 0.5298\n",
      "Epoch 32/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5344 - val_loss: 0.5297\n",
      "Epoch 33/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5342 - val_loss: 0.5296\n",
      "Epoch 34/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5340 - val_loss: 0.5290\n",
      "Epoch 35/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5338 - val_loss: 0.5290\n",
      "Epoch 36/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5337 - val_loss: 0.5285\n",
      "Epoch 37/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5335 - val_loss: 0.5286\n",
      "Epoch 38/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5334 - val_loss: 0.5286\n",
      "Epoch 39/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5332 - val_loss: 0.5283\n",
      "Epoch 40/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5331 - val_loss: 0.5283\n",
      "Epoch 41/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5330 - val_loss: 0.5281\n",
      "Epoch 42/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5329 - val_loss: 0.5282\n",
      "Epoch 43/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5327 - val_loss: 0.5277\n",
      "Epoch 44/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5326 - val_loss: 0.5280\n",
      "Epoch 45/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5325 - val_loss: 0.5277\n",
      "Epoch 46/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5323 - val_loss: 0.5276\n",
      "Epoch 47/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5323 - val_loss: 0.5276\n",
      "Epoch 48/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5322 - val_loss: 0.5274\n",
      "Epoch 49/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5321 - val_loss: 0.5274\n",
      "Epoch 50/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5320 - val_loss: 0.5272\n",
      "Epoch 51/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5319 - val_loss: 0.5272\n",
      "Epoch 52/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5318 - val_loss: 0.5273\n",
      "Epoch 53/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5318 - val_loss: 0.5271\n",
      "Epoch 54/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5317 - val_loss: 0.5269\n",
      "Epoch 55/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5316 - val_loss: 0.5266\n",
      "Epoch 56/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5316 - val_loss: 0.5271\n",
      "Epoch 57/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5315 - val_loss: 0.5269\n",
      "Epoch 58/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5314 - val_loss: 0.5266\n",
      "Epoch 59/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5313 - val_loss: 0.5264\n",
      "Epoch 60/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5313 - val_loss: 0.5265\n",
      "Epoch 61/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5312 - val_loss: 0.5265\n",
      "Epoch 62/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5311 - val_loss: 0.5263\n",
      "Epoch 63/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5311 - val_loss: 0.5265\n",
      "Epoch 64/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5310 - val_loss: 0.5263\n",
      "Epoch 65/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5309 - val_loss: 0.5265\n",
      "Epoch 66/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5309 - val_loss: 0.5261\n",
      "Epoch 67/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5308 - val_loss: 0.5260\n",
      "Epoch 68/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5308 - val_loss: 0.5259\n",
      "Epoch 69/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5307 - val_loss: 0.5259\n",
      "Epoch 70/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5306 - val_loss: 0.5259\n",
      "Epoch 71/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5306 - val_loss: 0.5259\n",
      "Epoch 72/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5306 - val_loss: 0.5258\n",
      "Epoch 73/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5305 - val_loss: 0.5257\n",
      "Epoch 74/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5305 - val_loss: 0.5260\n",
      "Epoch 75/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5304 - val_loss: 0.5257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5304 - val_loss: 0.5257\n",
      "Epoch 77/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5303 - val_loss: 0.5255\n",
      "Epoch 78/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5302 - val_loss: 0.5257\n",
      "Epoch 79/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5302 - val_loss: 0.5254\n",
      "Epoch 80/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5302 - val_loss: 0.5254\n",
      "Epoch 81/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5301 - val_loss: 0.5254\n",
      "Epoch 82/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5300 - val_loss: 0.5255\n",
      "Epoch 83/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5301 - val_loss: 0.5255\n",
      "Epoch 84/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5300 - val_loss: 0.5255\n",
      "Epoch 85/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5299 - val_loss: 0.5254\n",
      "Epoch 86/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5299 - val_loss: 0.5252\n",
      "Epoch 87/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5299 - val_loss: 0.5253\n",
      "Epoch 88/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5298 - val_loss: 0.5251\n",
      "Epoch 89/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5297 - val_loss: 0.5250\n",
      "Epoch 90/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5297 - val_loss: 0.5251\n",
      "Epoch 91/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5297 - val_loss: 0.5250\n",
      "Epoch 92/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5296 - val_loss: 0.5248\n",
      "Epoch 93/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5296 - val_loss: 0.5248\n",
      "Epoch 94/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5296 - val_loss: 0.5250\n",
      "Epoch 95/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5295 - val_loss: 0.5248\n",
      "Epoch 96/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5294 - val_loss: 0.5249\n",
      "Epoch 97/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5294 - val_loss: 0.5247\n",
      "Epoch 98/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5294 - val_loss: 0.5246\n",
      "Epoch 99/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5294 - val_loss: 0.5246\n",
      "Epoch 100/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5293 - val_loss: 0.5246\n",
      "Epoch 101/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5293 - val_loss: 0.5243\n",
      "Epoch 102/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5292 - val_loss: 0.5245\n",
      "Epoch 103/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5292 - val_loss: 0.5244\n",
      "Epoch 104/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5291 - val_loss: 0.5246\n",
      "Epoch 105/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5291 - val_loss: 0.5244\n",
      "Epoch 106/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5291 - val_loss: 0.5244\n",
      "Epoch 107/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5290 - val_loss: 0.5246\n",
      "Epoch 108/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5290 - val_loss: 0.5241\n",
      "Epoch 109/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5289 - val_loss: 0.5241\n",
      "Epoch 110/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5289 - val_loss: 0.5243\n",
      "Epoch 111/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5289 - val_loss: 0.5241\n",
      "Epoch 112/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5288 - val_loss: 0.5244\n",
      "Epoch 113/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5288 - val_loss: 0.5240\n",
      "Epoch 114/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5288 - val_loss: 0.5242\n",
      "Epoch 115/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5287 - val_loss: 0.5240\n",
      "Epoch 116/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5287 - val_loss: 0.5237\n",
      "Epoch 117/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5286 - val_loss: 0.5240\n",
      "Epoch 118/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5286 - val_loss: 0.5240\n",
      "Epoch 119/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5285 - val_loss: 0.5237\n",
      "Epoch 120/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5285 - val_loss: 0.5238\n",
      "Epoch 121/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5284 - val_loss: 0.5239\n",
      "Epoch 122/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5284 - val_loss: 0.5237\n",
      "Epoch 123/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5284 - val_loss: 0.5237\n",
      "Epoch 124/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5283 - val_loss: 0.5239\n",
      "Epoch 125/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5283 - val_loss: 0.5236\n",
      "Epoch 126/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5283 - val_loss: 0.5238\n",
      "Epoch 127/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5282 - val_loss: 0.5237\n",
      "Epoch 128/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5282 - val_loss: 0.5236\n",
      "Epoch 129/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5281 - val_loss: 0.5234\n",
      "Epoch 130/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5281 - val_loss: 0.5235\n",
      "Epoch 131/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5281 - val_loss: 0.5233\n",
      "Epoch 132/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5280 - val_loss: 0.5234\n",
      "Epoch 133/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5280 - val_loss: 0.5235\n",
      "Epoch 134/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5280 - val_loss: 0.5234\n",
      "Epoch 135/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5279 - val_loss: 0.5234\n",
      "Epoch 136/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5279 - val_loss: 0.5231\n",
      "Epoch 137/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5278 - val_loss: 0.5234\n",
      "Epoch 138/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5278 - val_loss: 0.5231\n",
      "Epoch 139/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5278 - val_loss: 0.5232\n",
      "Epoch 140/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5277 - val_loss: 0.5231\n",
      "Epoch 141/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5277 - val_loss: 0.5230\n",
      "Epoch 142/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5276 - val_loss: 0.5230\n",
      "Epoch 143/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5276 - val_loss: 0.5230\n",
      "Epoch 144/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5276 - val_loss: 0.5229\n",
      "Epoch 145/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5276 - val_loss: 0.5230\n",
      "Epoch 146/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5275 - val_loss: 0.5228\n",
      "Epoch 147/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5274 - val_loss: 0.5228\n",
      "Epoch 148/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5274 - val_loss: 0.5229\n",
      "Epoch 149/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5274 - val_loss: 0.5227\n",
      "Epoch 150/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5273 - val_loss: 0.5228\n",
      "Epoch 151/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5273 - val_loss: 0.5226\n",
      "Epoch 152/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5273 - val_loss: 0.5226\n",
      "Epoch 153/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5272 - val_loss: 0.5226\n",
      "Epoch 154/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5273 - val_loss: 0.5227\n",
      "Epoch 155/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5272 - val_loss: 0.5228\n",
      "Epoch 156/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5271 - val_loss: 0.5227\n",
      "Epoch 157/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5271 - val_loss: 0.5226\n",
      "Epoch 158/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5271 - val_loss: 0.5228\n",
      "Epoch 159/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5271 - val_loss: 0.5226\n",
      "Epoch 160/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5271 - val_loss: 0.5226\n",
      "Epoch 161/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5270 - val_loss: 0.5225\n",
      "Epoch 162/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5270 - val_loss: 0.5222\n",
      "Epoch 163/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5269 - val_loss: 0.5223\n",
      "Epoch 164/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5269 - val_loss: 0.5223\n",
      "Epoch 165/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5269 - val_loss: 0.5222\n",
      "Epoch 166/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5269 - val_loss: 0.5223\n",
      "Epoch 167/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5268 - val_loss: 0.5222\n",
      "Epoch 168/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5268 - val_loss: 0.5221\n",
      "Epoch 169/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5267 - val_loss: 0.5221\n",
      "Epoch 170/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5267 - val_loss: 0.5222\n",
      "Epoch 171/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5267 - val_loss: 0.5220\n",
      "Epoch 172/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5267 - val_loss: 0.5218\n",
      "Epoch 173/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5266 - val_loss: 0.5221\n",
      "Epoch 174/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5266 - val_loss: 0.5223\n",
      "Epoch 175/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5266 - val_loss: 0.5220\n",
      "Epoch 176/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5266 - val_loss: 0.5219\n",
      "Epoch 177/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5265 - val_loss: 0.5218\n",
      "Epoch 178/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5265 - val_loss: 0.5218\n",
      "Epoch 179/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5265 - val_loss: 0.5218\n",
      "Epoch 180/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5265 - val_loss: 0.5218\n",
      "Epoch 181/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5264 - val_loss: 0.5218\n",
      "Epoch 182/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5264 - val_loss: 0.5220\n",
      "Epoch 183/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5264 - val_loss: 0.5219\n",
      "Epoch 184/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5264 - val_loss: 0.5219\n",
      "Epoch 185/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5264 - val_loss: 0.5218\n",
      "Epoch 186/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5263 - val_loss: 0.5216\n",
      "Epoch 187/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5263 - val_loss: 0.5219\n",
      "Epoch 188/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5263 - val_loss: 0.5219\n",
      "Epoch 189/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5263 - val_loss: 0.5218\n",
      "Epoch 190/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5262 - val_loss: 0.5218\n",
      "Epoch 191/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5262 - val_loss: 0.5218\n",
      "Epoch 192/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5262 - val_loss: 0.5217\n",
      "Epoch 193/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5262 - val_loss: 0.5216\n",
      "Epoch 194/500\n",
      "4909/4909 [==============================] - 47s 9ms/step - loss: 0.5262 - val_loss: 0.5216\n",
      "Epoch 195/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5262 - val_loss: 0.5216\n",
      "Epoch 196/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5261 - val_loss: 0.5219\n",
      "Epoch 197/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5261 - val_loss: 0.5215\n",
      "Epoch 198/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5261 - val_loss: 0.5217\n",
      "Epoch 199/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5261 - val_loss: 0.5218\n",
      "Epoch 200/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5261 - val_loss: 0.5212\n",
      "Epoch 201/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5260 - val_loss: 0.5213\n",
      "Epoch 202/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5260 - val_loss: 0.5214\n",
      "Epoch 203/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5260 - val_loss: 0.5213\n",
      "Epoch 204/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5260 - val_loss: 0.5213\n",
      "Epoch 205/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5260 - val_loss: 0.5217\n",
      "Epoch 206/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5260 - val_loss: 0.5217\n",
      "Epoch 207/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5260 - val_loss: 0.5213\n",
      "Epoch 208/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5259 - val_loss: 0.5215\n",
      "Epoch 209/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5259 - val_loss: 0.5214\n",
      "Epoch 210/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5259 - val_loss: 0.5212\n",
      "Epoch 211/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5259 - val_loss: 0.5214\n",
      "Epoch 212/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5259 - val_loss: 0.5210\n",
      "Epoch 213/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5258 - val_loss: 0.5216\n",
      "Epoch 214/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5259 - val_loss: 0.5213\n",
      "Epoch 215/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5258 - val_loss: 0.5216\n",
      "Epoch 216/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5258 - val_loss: 0.5214\n",
      "Epoch 217/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5258 - val_loss: 0.5215\n",
      "Epoch 218/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5258 - val_loss: 0.5214\n",
      "Epoch 219/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5257 - val_loss: 0.5214\n",
      "Epoch 220/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5257 - val_loss: 0.5212\n",
      "Epoch 221/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5258 - val_loss: 0.5212\n",
      "Epoch 222/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5257 - val_loss: 0.5213\n",
      "Epoch 223/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5257 - val_loss: 0.5211\n",
      "Epoch 224/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5256 - val_loss: 0.5212\n",
      "Epoch 225/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5257 - val_loss: 0.5212\n",
      "Epoch 226/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5257 - val_loss: 0.5213\n",
      "Epoch 227/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5256 - val_loss: 0.5210\n",
      "Epoch 228/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5256 - val_loss: 0.5210\n",
      "Epoch 229/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5256 - val_loss: 0.5212\n",
      "Epoch 230/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5256 - val_loss: 0.5212\n",
      "Epoch 231/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5256 - val_loss: 0.5212\n",
      "Epoch 232/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5256 - val_loss: 0.5212\n",
      "Epoch 233/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5256 - val_loss: 0.5211\n",
      "Epoch 234/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5255 - val_loss: 0.5207\n",
      "Epoch 235/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5255 - val_loss: 0.5209\n",
      "Epoch 236/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5255 - val_loss: 0.5208\n",
      "Epoch 237/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5255 - val_loss: 0.5208\n",
      "Epoch 238/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5255 - val_loss: 0.5211\n",
      "Epoch 239/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5254 - val_loss: 0.5210\n",
      "Epoch 240/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5254 - val_loss: 0.5209\n",
      "Epoch 241/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5255 - val_loss: 0.5209\n",
      "Epoch 242/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5254 - val_loss: 0.5208\n",
      "Epoch 243/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5254 - val_loss: 0.5210\n",
      "Epoch 244/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5254 - val_loss: 0.5208\n",
      "Epoch 245/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5254 - val_loss: 0.5208\n",
      "Epoch 246/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5254 - val_loss: 0.5210\n",
      "Epoch 247/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5254 - val_loss: 0.5209\n",
      "Epoch 248/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5253 - val_loss: 0.5209\n",
      "Epoch 249/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5253 - val_loss: 0.5207\n",
      "Epoch 250/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5253 - val_loss: 0.5208\n",
      "Epoch 251/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5253 - val_loss: 0.5207\n",
      "Epoch 252/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5252 - val_loss: 0.5209\n",
      "Epoch 253/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5253 - val_loss: 0.5206\n",
      "Epoch 254/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5252 - val_loss: 0.5207\n",
      "Epoch 255/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5253 - val_loss: 0.5205\n",
      "Epoch 256/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5252 - val_loss: 0.5209\n",
      "Epoch 257/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5252 - val_loss: 0.5207\n",
      "Epoch 258/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5252 - val_loss: 0.5204\n",
      "Epoch 259/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5251 - val_loss: 0.5208\n",
      "Epoch 260/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5252 - val_loss: 0.5206\n",
      "Epoch 261/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5252 - val_loss: 0.5205\n",
      "Epoch 262/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5251 - val_loss: 0.5206\n",
      "Epoch 263/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5251 - val_loss: 0.5205\n",
      "Epoch 264/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5251 - val_loss: 0.5205\n",
      "Epoch 265/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5251 - val_loss: 0.5207\n",
      "Epoch 266/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5250 - val_loss: 0.5207\n",
      "Epoch 267/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5250 - val_loss: 0.5204\n",
      "Epoch 268/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5251 - val_loss: 0.5205\n",
      "Epoch 269/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5251 - val_loss: 0.5206\n",
      "Epoch 270/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5250 - val_loss: 0.5204\n",
      "Epoch 271/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5250 - val_loss: 0.5207\n",
      "Epoch 272/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5250 - val_loss: 0.5204\n",
      "Epoch 273/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5250 - val_loss: 0.5205\n",
      "Epoch 274/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5250 - val_loss: 0.5205\n",
      "Epoch 275/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5249 - val_loss: 0.5204\n",
      "Epoch 276/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5249 - val_loss: 0.5204\n",
      "Epoch 277/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5249 - val_loss: 0.5203\n",
      "Epoch 278/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5249 - val_loss: 0.5204\n",
      "Epoch 279/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5249 - val_loss: 0.5203\n",
      "Epoch 280/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5249 - val_loss: 0.5202\n",
      "Epoch 281/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5249 - val_loss: 0.5203\n",
      "Epoch 282/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5249 - val_loss: 0.5203\n",
      "Epoch 283/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5248 - val_loss: 0.5202\n",
      "Epoch 284/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5248 - val_loss: 0.5203\n",
      "Epoch 285/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5248 - val_loss: 0.5203\n",
      "Epoch 286/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5248 - val_loss: 0.5203\n",
      "Epoch 287/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5248 - val_loss: 0.5204\n",
      "Epoch 288/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5248 - val_loss: 0.5202\n",
      "Epoch 289/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5247 - val_loss: 0.5204\n",
      "Epoch 290/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5247 - val_loss: 0.5204\n",
      "Epoch 291/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5247 - val_loss: 0.5201\n",
      "Epoch 292/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5247 - val_loss: 0.5202\n",
      "Epoch 293/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5247 - val_loss: 0.5201\n",
      "Epoch 294/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5247 - val_loss: 0.5201\n",
      "Epoch 295/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5247 - val_loss: 0.5202\n",
      "Epoch 296/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5247 - val_loss: 0.5202\n",
      "Epoch 297/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5247 - val_loss: 0.5200\n",
      "Epoch 298/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5246 - val_loss: 0.5203\n",
      "Epoch 299/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5246 - val_loss: 0.5200\n",
      "Epoch 300/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5246 - val_loss: 0.5200\n",
      "Epoch 301/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5246 - val_loss: 0.5201\n",
      "Epoch 302/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5245 - val_loss: 0.5202\n",
      "Epoch 303/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5246 - val_loss: 0.5200\n",
      "Epoch 304/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5246 - val_loss: 0.5201\n",
      "Epoch 305/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5245 - val_loss: 0.5199\n",
      "Epoch 306/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5246 - val_loss: 0.5200\n",
      "Epoch 307/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5245 - val_loss: 0.5202\n",
      "Epoch 308/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5245 - val_loss: 0.5202\n",
      "Epoch 309/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5245 - val_loss: 0.5202\n",
      "Epoch 310/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5244 - val_loss: 0.5200\n",
      "Epoch 311/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5245 - val_loss: 0.5198\n",
      "Epoch 312/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5244 - val_loss: 0.5199\n",
      "Epoch 313/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5244 - val_loss: 0.5201\n",
      "Epoch 314/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5244 - val_loss: 0.5201\n",
      "Epoch 315/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5244 - val_loss: 0.5198\n",
      "Epoch 316/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5244 - val_loss: 0.5200\n",
      "Epoch 317/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5244 - val_loss: 0.5197\n",
      "Epoch 318/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5244 - val_loss: 0.5200\n",
      "Epoch 319/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5244 - val_loss: 0.5196\n",
      "Epoch 320/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5244 - val_loss: 0.5197\n",
      "Epoch 321/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5244 - val_loss: 0.5199\n",
      "Epoch 322/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5243 - val_loss: 0.5202\n",
      "Epoch 323/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5243 - val_loss: 0.5198\n",
      "Epoch 324/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5243 - val_loss: 0.5198\n",
      "Epoch 325/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5243 - val_loss: 0.5198\n",
      "Epoch 326/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5242 - val_loss: 0.5199\n",
      "Epoch 327/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5243 - val_loss: 0.5197\n",
      "Epoch 328/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5242 - val_loss: 0.5197\n",
      "Epoch 329/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5242 - val_loss: 0.5195\n",
      "Epoch 330/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5243 - val_loss: 0.5196\n",
      "Epoch 331/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5242 - val_loss: 0.5195\n",
      "Epoch 332/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5242 - val_loss: 0.5196\n",
      "Epoch 333/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5242 - val_loss: 0.5198\n",
      "Epoch 334/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5242 - val_loss: 0.5194\n",
      "Epoch 335/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5242 - val_loss: 0.5196\n",
      "Epoch 336/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5241 - val_loss: 0.5199\n",
      "Epoch 337/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5242 - val_loss: 0.5197\n",
      "Epoch 338/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5242 - val_loss: 0.5195\n",
      "Epoch 339/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5241 - val_loss: 0.5194\n",
      "Epoch 340/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5241 - val_loss: 0.5195\n",
      "Epoch 341/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5241 - val_loss: 0.5194\n",
      "Epoch 342/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5241 - val_loss: 0.5196\n",
      "Epoch 343/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5241 - val_loss: 0.5195\n",
      "Epoch 344/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5241 - val_loss: 0.5198\n",
      "Epoch 345/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5241 - val_loss: 0.5195\n",
      "Epoch 346/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5240 - val_loss: 0.5196\n",
      "Epoch 347/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5240 - val_loss: 0.5197\n",
      "Epoch 348/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5240 - val_loss: 0.5196\n",
      "Epoch 349/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5240 - val_loss: 0.5196\n",
      "Epoch 350/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5241 - val_loss: 0.5195\n",
      "Epoch 351/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5240 - val_loss: 0.5195\n",
      "Epoch 352/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5240 - val_loss: 0.5196\n",
      "Epoch 353/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5240 - val_loss: 0.5198\n",
      "Epoch 354/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5240 - val_loss: 0.5195\n",
      "Epoch 355/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5240 - val_loss: 0.5194\n",
      "Epoch 356/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5239 - val_loss: 0.5196\n",
      "Epoch 357/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5239 - val_loss: 0.5196\n",
      "Epoch 358/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5239 - val_loss: 0.5193\n",
      "Epoch 359/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5239 - val_loss: 0.5193\n",
      "Epoch 360/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5239 - val_loss: 0.5194\n",
      "Epoch 361/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5239 - val_loss: 0.5194\n",
      "Epoch 362/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5239 - val_loss: 0.5194\n",
      "Epoch 363/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5239 - val_loss: 0.5197\n",
      "Epoch 364/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5239 - val_loss: 0.5193\n",
      "Epoch 365/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5239 - val_loss: 0.5193\n",
      "Epoch 366/500\n",
      "4909/4909 [==============================] - 47s 10ms/step - loss: 0.5239 - val_loss: 0.5195\n",
      "Epoch 367/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5238 - val_loss: 0.5196\n",
      "Epoch 368/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5238 - val_loss: 0.5193\n",
      "Epoch 369/500\n",
      "4909/4909 [==============================] - 47s 9ms/step - loss: 0.5238 - val_loss: 0.5194\n",
      "Epoch 370/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5238 - val_loss: 0.5193\n",
      "Epoch 371/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5238 - val_loss: 0.5191\n",
      "Epoch 372/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5238 - val_loss: 0.5193\n",
      "Epoch 373/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5238 - val_loss: 0.5194\n",
      "Epoch 374/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5238 - val_loss: 0.5196\n",
      "Epoch 375/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5238 - val_loss: 0.5193\n",
      "Epoch 376/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5237 - val_loss: 0.5194\n",
      "Epoch 377/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5237 - val_loss: 0.5197\n",
      "Epoch 378/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5237 - val_loss: 0.5193\n",
      "Epoch 379/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5237 - val_loss: 0.5194\n",
      "Epoch 380/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5238 - val_loss: 0.5192\n",
      "Epoch 381/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5237 - val_loss: 0.5196\n",
      "Epoch 382/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5237 - val_loss: 0.5191\n",
      "Epoch 383/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5237 - val_loss: 0.5191\n",
      "Epoch 384/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5237 - val_loss: 0.5193\n",
      "Epoch 385/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5237 - val_loss: 0.5193\n",
      "Epoch 386/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5237 - val_loss: 0.5194\n",
      "Epoch 387/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5237 - val_loss: 0.5196\n",
      "Epoch 388/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5236 - val_loss: 0.5193\n",
      "Epoch 389/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5237 - val_loss: 0.5194\n",
      "Epoch 390/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5237 - val_loss: 0.5191\n",
      "Epoch 391/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5236 - val_loss: 0.5193\n",
      "Epoch 392/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5236 - val_loss: 0.5190\n",
      "Epoch 393/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5236 - val_loss: 0.5192\n",
      "Epoch 394/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5236 - val_loss: 0.5192\n",
      "Epoch 395/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5236 - val_loss: 0.5193\n",
      "Epoch 396/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5236 - val_loss: 0.5189\n",
      "Epoch 397/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5236 - val_loss: 0.5193\n",
      "Epoch 398/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5236 - val_loss: 0.5192\n",
      "Epoch 399/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5235 - val_loss: 0.5191\n",
      "Epoch 400/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5235 - val_loss: 0.5194\n",
      "Epoch 401/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5236 - val_loss: 0.5195\n",
      "Epoch 402/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5236 - val_loss: 0.5191\n",
      "Epoch 403/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5235 - val_loss: 0.5190\n",
      "Epoch 404/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5236 - val_loss: 0.5191\n",
      "Epoch 405/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5235 - val_loss: 0.5192\n",
      "Epoch 406/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5235 - val_loss: 0.5192\n",
      "Epoch 407/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5235 - val_loss: 0.5187\n",
      "Epoch 408/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5235 - val_loss: 0.5193\n",
      "Epoch 409/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5235 - val_loss: 0.5190\n",
      "Epoch 410/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5235 - val_loss: 0.5190\n",
      "Epoch 411/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5234 - val_loss: 0.5192\n",
      "Epoch 412/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5235 - val_loss: 0.5191\n",
      "Epoch 413/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5235 - val_loss: 0.5187\n",
      "Epoch 414/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5235 - val_loss: 0.5193\n",
      "Epoch 415/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5234 - val_loss: 0.5190\n",
      "Epoch 416/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5234 - val_loss: 0.5191\n",
      "Epoch 417/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5234 - val_loss: 0.5189\n",
      "Epoch 418/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5234 - val_loss: 0.5190\n",
      "Epoch 419/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5235 - val_loss: 0.5190\n",
      "Epoch 420/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5234 - val_loss: 0.5191\n",
      "Epoch 421/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5234 - val_loss: 0.5187\n",
      "Epoch 422/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5234 - val_loss: 0.5189\n",
      "Epoch 423/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5233 - val_loss: 0.5191\n",
      "Epoch 424/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5234 - val_loss: 0.5187\n",
      "Epoch 425/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5234 - val_loss: 0.5190\n",
      "Epoch 426/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5234 - val_loss: 0.5190\n",
      "Epoch 427/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5234 - val_loss: 0.5188\n",
      "Epoch 428/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5234 - val_loss: 0.5190\n",
      "Epoch 429/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5233 - val_loss: 0.5192\n",
      "Epoch 430/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5234 - val_loss: 0.5190\n",
      "Epoch 431/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5233 - val_loss: 0.5190\n",
      "Epoch 432/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5234 - val_loss: 0.5192\n",
      "Epoch 433/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5233 - val_loss: 0.5187\n",
      "Epoch 434/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5233 - val_loss: 0.5186\n",
      "Epoch 435/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5233 - val_loss: 0.5191\n",
      "Epoch 436/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5234 - val_loss: 0.5191\n",
      "Epoch 437/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5233 - val_loss: 0.5188\n",
      "Epoch 438/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5233 - val_loss: 0.5188\n",
      "Epoch 439/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5233 - val_loss: 0.5190\n",
      "Epoch 440/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5233 - val_loss: 0.5189\n",
      "Epoch 441/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5233 - val_loss: 0.5189\n",
      "Epoch 442/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5233 - val_loss: 0.5189\n",
      "Epoch 443/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5232 - val_loss: 0.5189\n",
      "Epoch 444/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5233 - val_loss: 0.5189\n",
      "Epoch 445/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5232 - val_loss: 0.5191\n",
      "Epoch 446/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5232 - val_loss: 0.5188\n",
      "Epoch 447/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5232 - val_loss: 0.5190\n",
      "Epoch 448/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5232 - val_loss: 0.5188\n",
      "Epoch 449/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5232 - val_loss: 0.5188\n",
      "Epoch 450/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5232 - val_loss: 0.5188\n",
      "Epoch 451/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5232 - val_loss: 0.5191\n",
      "Epoch 452/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5232 - val_loss: 0.5189\n",
      "Epoch 453/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5232 - val_loss: 0.5189\n",
      "Epoch 454/500\n",
      "4909/4909 [==============================] - 47s 9ms/step - loss: 0.5232 - val_loss: 0.5188\n",
      "Epoch 455/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5188\n",
      "Epoch 456/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5232 - val_loss: 0.5188\n",
      "Epoch 457/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5232 - val_loss: 0.5187\n",
      "Epoch 458/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5231 - val_loss: 0.5189\n",
      "Epoch 459/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5232 - val_loss: 0.5187\n",
      "Epoch 460/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5232 - val_loss: 0.5186\n",
      "Epoch 461/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5190\n",
      "Epoch 462/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5232 - val_loss: 0.5187\n",
      "Epoch 463/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5232 - val_loss: 0.5191\n",
      "Epoch 464/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5188\n",
      "Epoch 465/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5188\n",
      "Epoch 466/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5187\n",
      "Epoch 467/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5188\n",
      "Epoch 468/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5186\n",
      "Epoch 469/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5185\n",
      "Epoch 470/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5186\n",
      "Epoch 471/500\n",
      "4909/4909 [==============================] - 47s 10ms/step - loss: 0.5231 - val_loss: 0.5187\n",
      "Epoch 472/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5187\n",
      "Epoch 473/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5189\n",
      "Epoch 474/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5188\n",
      "Epoch 475/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5230 - val_loss: 0.5186\n",
      "Epoch 476/500\n",
      "4909/4909 [==============================] - 47s 9ms/step - loss: 0.5231 - val_loss: 0.5189\n",
      "Epoch 477/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5189\n",
      "Epoch 478/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5186\n",
      "Epoch 479/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5189\n",
      "Epoch 480/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5188\n",
      "Epoch 481/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5230 - val_loss: 0.5189\n",
      "Epoch 482/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5231 - val_loss: 0.5190\n",
      "Epoch 483/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5230 - val_loss: 0.5186\n",
      "Epoch 484/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5230 - val_loss: 0.5186\n",
      "Epoch 485/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5230 - val_loss: 0.5184\n",
      "Epoch 486/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5230 - val_loss: 0.5186\n",
      "Epoch 487/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5230 - val_loss: 0.5188\n",
      "Epoch 488/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5230 - val_loss: 0.5187\n",
      "Epoch 489/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5230 - val_loss: 0.5187\n",
      "Epoch 490/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5230 - val_loss: 0.5186\n",
      "Epoch 491/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5230 - val_loss: 0.5183\n",
      "Epoch 492/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5186\n",
      "Epoch 493/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5230 - val_loss: 0.5186\n",
      "Epoch 494/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5187\n",
      "Epoch 495/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5187\n",
      "Epoch 496/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5229 - val_loss: 0.5185\n",
      "Epoch 497/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5230 - val_loss: 0.5187\n",
      "Epoch 498/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5230 - val_loss: 0.5186\n",
      "Epoch 499/500\n",
      "4909/4909 [==============================] - 45s 9ms/step - loss: 0.5229 - val_loss: 0.5185\n",
      "Epoch 500/500\n",
      "4909/4909 [==============================] - 46s 9ms/step - loss: 0.5230 - val_loss: 0.5186\n"
     ]
    }
   ],
   "source": [
    "%%wandb\n",
    "\n",
    "model_version = 'patch_desc_ae_' + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + '8_alex_3conv3mp_2020_augm_elu_lastelu_NOTdwnsmpl'\n",
    "\n",
    "os.system('mkdir ' + base_dir + '/weights_' + model_version)\n",
    "print(base_dir + '/weights_' + model_version)\n",
    "\n",
    "# checkpointer = ModelCheckpoint(base_dir + '/weights' + model_version + '/weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "history_callback = autoencoder.fit(image_datagen.flow(x_train, y_train, batch_size),\n",
    "                epochs=wandb.config.epochs,\n",
    "                validation_data=image_datagen.flow(x_validation, y_validation, batch_size),\n",
    "                callbacks=[WandbCallback(data_type=\"image\", predictions=1)]\n",
    "                )\n",
    "autoencoder.save(base_dir + '/' + model_version + '.h5')\n",
    "\n",
    "# autoencoder = load_model(base_dir + '/' + model_version + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.068377174709958,\n",
       "  0.5947456061297459,\n",
       "  0.574823867837119,\n",
       "  0.568273215027032,\n",
       "  0.5641010319071215,\n",
       "  0.5602473130393757,\n",
       "  0.5563150000089861,\n",
       "  0.552475645503958,\n",
       "  0.5498284583943623,\n",
       "  0.5479230257507806,\n",
       "  0.5464430404980078,\n",
       "  0.5453583050992626,\n",
       "  0.5442801084475997,\n",
       "  0.5432381827760875,\n",
       "  0.5422809018091335,\n",
       "  0.5413961509396825,\n",
       "  0.5405217153687601,\n",
       "  0.5397875459051241,\n",
       "  0.5391285980556586,\n",
       "  0.5385475417734803,\n",
       "  0.5380775990474974,\n",
       "  0.5376462955244337,\n",
       "  0.5372382814741851,\n",
       "  0.5368034366241315,\n",
       "  0.5363939960632192,\n",
       "  0.5360575693829889,\n",
       "  0.5357197200988045,\n",
       "  0.535451823111219,\n",
       "  0.53517677905068,\n",
       "  0.534911313516432,\n",
       "  0.5346473203041051,\n",
       "  0.5344342031898147,\n",
       "  0.534226682672629,\n",
       "  0.5340069451179162,\n",
       "  0.5338153242677852,\n",
       "  0.5336572671996587,\n",
       "  0.5334905259154775,\n",
       "  0.5333664291201842,\n",
       "  0.5331998471998648,\n",
       "  0.5330723661587352,\n",
       "  0.5329699229971285,\n",
       "  0.53286920406434,\n",
       "  0.5326796183193871,\n",
       "  0.5326212196762442,\n",
       "  0.5324641826843594,\n",
       "  0.5323325149054627,\n",
       "  0.5322782588189018,\n",
       "  0.5321826023484075,\n",
       "  0.5320746111260888,\n",
       "  0.5320353001018803,\n",
       "  0.5319337097102244,\n",
       "  0.5318487625839788,\n",
       "  0.5317702501344059,\n",
       "  0.5316917071627139,\n",
       "  0.5315977587561531,\n",
       "  0.531587444822848,\n",
       "  0.5314843042004611,\n",
       "  0.531406675039164,\n",
       "  0.5313285494805985,\n",
       "  0.5313105212402567,\n",
       "  0.5311952903511044,\n",
       "  0.5311362423024443,\n",
       "  0.5310683771450901,\n",
       "  0.5310260897407135,\n",
       "  0.5309487327719118,\n",
       "  0.5308717429587064,\n",
       "  0.5308253238869046,\n",
       "  0.5308081553519101,\n",
       "  0.5307282038481601,\n",
       "  0.5306257841526144,\n",
       "  0.5305838272029207,\n",
       "  0.5305993467466523,\n",
       "  0.5305147983043961,\n",
       "  0.5305051651463953,\n",
       "  0.5304292588798069,\n",
       "  0.5304036353034298,\n",
       "  0.5303346268802234,\n",
       "  0.5302276711750602,\n",
       "  0.530232001704489,\n",
       "  0.5301662869412329,\n",
       "  0.5300941967523112,\n",
       "  0.5300294479321707,\n",
       "  0.5300506314812058,\n",
       "  0.5300137611345108,\n",
       "  0.529941148764643,\n",
       "  0.5298580020905549,\n",
       "  0.5298766010080146,\n",
       "  0.5298094543771669,\n",
       "  0.5297428294241283,\n",
       "  0.52969316683911,\n",
       "  0.5296792601260503,\n",
       "  0.5296417441143535,\n",
       "  0.529609428565606,\n",
       "  0.529587266240809,\n",
       "  0.5295089230721306,\n",
       "  0.5294427914421409,\n",
       "  0.5294356862137836,\n",
       "  0.5293729225493958,\n",
       "  0.529351880973748,\n",
       "  0.529271204969595,\n",
       "  0.5292858312397191,\n",
       "  0.529210944541682,\n",
       "  0.529159533054408,\n",
       "  0.5291450440290952,\n",
       "  0.5291028417717176,\n",
       "  0.529119588758453,\n",
       "  0.5290429281272904,\n",
       "  0.5289737627891251,\n",
       "  0.5289369626433045,\n",
       "  0.5288993947599414,\n",
       "  0.5288779155720807,\n",
       "  0.5288410434503675,\n",
       "  0.5287606892883782,\n",
       "  0.5287842880279706,\n",
       "  0.5287258322685477,\n",
       "  0.5286549428480278,\n",
       "  0.5286476260176038,\n",
       "  0.5286047917064808,\n",
       "  0.528531180285118,\n",
       "  0.5284780038065912,\n",
       "  0.5284440319912062,\n",
       "  0.5284285104493459,\n",
       "  0.528379262534865,\n",
       "  0.5283319860510488,\n",
       "  0.5282705474549139,\n",
       "  0.5282808586205867,\n",
       "  0.5282249350564668,\n",
       "  0.5281950429419808,\n",
       "  0.5281337475850435,\n",
       "  0.5280908696497527,\n",
       "  0.5280685480362229,\n",
       "  0.5280292036478929,\n",
       "  0.528002413795689,\n",
       "  0.5279661860268755,\n",
       "  0.527902585580603,\n",
       "  0.5278503966580698,\n",
       "  0.5277968891913679,\n",
       "  0.5277963916132837,\n",
       "  0.5277565133549921,\n",
       "  0.5277081397208914,\n",
       "  0.527680905270992,\n",
       "  0.5276491474027981,\n",
       "  0.527584236655302,\n",
       "  0.5275709639683146,\n",
       "  0.5275670625774542,\n",
       "  0.5275178861531833,\n",
       "  0.527433308726297,\n",
       "  0.527371800586477,\n",
       "  0.5273754128125099,\n",
       "  0.5273429470193511,\n",
       "  0.5273477005381505,\n",
       "  0.5272736448393394,\n",
       "  0.5272330290347532,\n",
       "  0.5272558538208243,\n",
       "  0.5271664664342609,\n",
       "  0.5271348870392645,\n",
       "  0.5271179312560386,\n",
       "  0.5270531717986892,\n",
       "  0.5270589211968686,\n",
       "  0.5270540501830877,\n",
       "  0.5270010669728663,\n",
       "  0.5269654014993506,\n",
       "  0.5269220099106388,\n",
       "  0.5269242912151839,\n",
       "  0.5269003874304587,\n",
       "  0.5268573837435814,\n",
       "  0.5267897535883324,\n",
       "  0.5268080049770871,\n",
       "  0.5267338370081772,\n",
       "  0.5267334292836606,\n",
       "  0.5267163632178405,\n",
       "  0.5266606753289517,\n",
       "  0.5266360306057423,\n",
       "  0.5265810306324517,\n",
       "  0.5265565230379452,\n",
       "  0.5265879566021715,\n",
       "  0.5265287756009206,\n",
       "  0.5265026793627385,\n",
       "  0.5264812602507866,\n",
       "  0.5264907925158048,\n",
       "  0.5264416048935132,\n",
       "  0.526426297256159,\n",
       "  0.526432566006183,\n",
       "  0.5263884000883045,\n",
       "  0.5264007373694732,\n",
       "  0.5263254906591692,\n",
       "  0.526332093025367,\n",
       "  0.5263104999995634,\n",
       "  0.5262998565055348,\n",
       "  0.5262408357545165,\n",
       "  0.5262374505137896,\n",
       "  0.5262412080636611,\n",
       "  0.526201727332827,\n",
       "  0.526211719439905,\n",
       "  0.5261777005178558,\n",
       "  0.5261340018540096,\n",
       "  0.5260941878340788,\n",
       "  0.52610600521018,\n",
       "  0.5260738065204732,\n",
       "  0.5260833620322364,\n",
       "  0.5260353584314017,\n",
       "  0.5260292237351021,\n",
       "  0.5259933464144018,\n",
       "  0.5259830796128038,\n",
       "  0.5259975612299383,\n",
       "  0.5259610952216413,\n",
       "  0.5259657307005838,\n",
       "  0.5259427087023125,\n",
       "  0.5258708836581757,\n",
       "  0.5258799014850917,\n",
       "  0.5259263438264048,\n",
       "  0.525881814387961,\n",
       "  0.5258459715856375,\n",
       "  0.5258837073080246,\n",
       "  0.5257766603430909,\n",
       "  0.525833018924057,\n",
       "  0.52577385224665,\n",
       "  0.5257782501235674,\n",
       "  0.5257328003559569,\n",
       "  0.5257479765911104,\n",
       "  0.525767751630854,\n",
       "  0.5257418392448049,\n",
       "  0.5256762449687908,\n",
       "  0.5256483125026117,\n",
       "  0.5256882375890588,\n",
       "  0.5256583593134929,\n",
       "  0.5256360941947211,\n",
       "  0.5255915298675675,\n",
       "  0.525563761262368,\n",
       "  0.5256144098294646,\n",
       "  0.5255679033712114,\n",
       "  0.52555738480918,\n",
       "  0.5255540777724669,\n",
       "  0.5255080514297298,\n",
       "  0.525500332218816,\n",
       "  0.525519256453347,\n",
       "  0.525476258170448,\n",
       "  0.5254623459297124,\n",
       "  0.5254472142809854,\n",
       "  0.5254476849848824,\n",
       "  0.525456135246085,\n",
       "  0.5254105899393845,\n",
       "  0.5253692627189895,\n",
       "  0.5253513489639146,\n",
       "  0.5253624172595172,\n",
       "  0.5253745821999907,\n",
       "  0.5253514661781109,\n",
       "  0.5253300974238613,\n",
       "  0.5253461882677505,\n",
       "  0.5253037546448166,\n",
       "  0.5252599968650438,\n",
       "  0.5252411437018215,\n",
       "  0.5252727833328501,\n",
       "  0.5252151039796461,\n",
       "  0.5252738120307178,\n",
       "  0.5251910040891664,\n",
       "  0.5252069419764104,\n",
       "  0.5251950305698708,\n",
       "  0.5251364118153494,\n",
       "  0.5251578250877263,\n",
       "  0.5251590644117421,\n",
       "  0.5250904765320388,\n",
       "  0.5251151264072786,\n",
       "  0.525137483143253,\n",
       "  0.525118966411646,\n",
       "  0.5250406396150264,\n",
       "  0.5250199769788504,\n",
       "  0.5250970983202304,\n",
       "  0.5250581678245867,\n",
       "  0.5250233083801373,\n",
       "  0.5250179147885166,\n",
       "  0.5250210236166211,\n",
       "  0.5249608372887035,\n",
       "  0.5250025045142755,\n",
       "  0.5249308514116634,\n",
       "  0.5249059991380431,\n",
       "  0.524948751743684,\n",
       "  0.5248577069976973,\n",
       "  0.5248727097872276,\n",
       "  0.5248819182696282,\n",
       "  0.5248684688169212,\n",
       "  0.5248552635290262,\n",
       "  0.5248370236243897,\n",
       "  0.5248435672100853,\n",
       "  0.5248147826675711,\n",
       "  0.5248147177264881,\n",
       "  0.524769491343302,\n",
       "  0.524831866787126,\n",
       "  0.5247388181205819,\n",
       "  0.5247488997898178,\n",
       "  0.524745604907313,\n",
       "  0.5246787939104985,\n",
       "  0.5247093878098581,\n",
       "  0.5246703963329628,\n",
       "  0.5246993173044934,\n",
       "  0.5246672788999277,\n",
       "  0.5246520974178838,\n",
       "  0.524647195841763,\n",
       "  0.5246260852270849,\n",
       "  0.5245622159440804,\n",
       "  0.5245764115783675,\n",
       "  0.5245390391595062,\n",
       "  0.5245715312447918,\n",
       "  0.5245549906200626,\n",
       "  0.5245381146883771,\n",
       "  0.5245702192816231,\n",
       "  0.5245145670919269,\n",
       "  0.5245036448949787,\n",
       "  0.5245128276665538,\n",
       "  0.52443723433137,\n",
       "  0.5244736066757673,\n",
       "  0.5244435881578174,\n",
       "  0.5244465086518486,\n",
       "  0.5244428925911996,\n",
       "  0.5244044197338305,\n",
       "  0.5244394196293555,\n",
       "  0.5244212044133342,\n",
       "  0.5244416420858584,\n",
       "  0.5243534509531249,\n",
       "  0.5243621938495096,\n",
       "  0.5243664037702755,\n",
       "  0.5243284895352291,\n",
       "  0.5243435637550179,\n",
       "  0.5243344017243686,\n",
       "  0.5242855658372492,\n",
       "  0.5242333301897681,\n",
       "  0.5242757521479707,\n",
       "  0.524229770115428,\n",
       "  0.5242333903901331,\n",
       "  0.5242516899828912,\n",
       "  0.5242453988353079,\n",
       "  0.5241945004543029,\n",
       "  0.5241945440845417,\n",
       "  0.5242055705985351,\n",
       "  0.5241583907048566,\n",
       "  0.5241491719267426,\n",
       "  0.5241677678557363,\n",
       "  0.5241903395275335,\n",
       "  0.5241153908359002,\n",
       "  0.5241249244843551,\n",
       "  0.5241177831902664,\n",
       "  0.5241332909190084,\n",
       "  0.524060362828855,\n",
       "  0.5240793671721864,\n",
       "  0.5240750276758417,\n",
       "  0.5240303208622984,\n",
       "  0.524049309688069,\n",
       "  0.5240363884812702,\n",
       "  0.523990770835174,\n",
       "  0.5240556484348289,\n",
       "  0.5240414412208048,\n",
       "  0.5239743993554998,\n",
       "  0.5240308697914577,\n",
       "  0.5239814292134981,\n",
       "  0.5239668188563109,\n",
       "  0.5239352151460678,\n",
       "  0.5239118003671279,\n",
       "  0.5238919907437439,\n",
       "  0.5239023380517951,\n",
       "  0.5239261943929137,\n",
       "  0.5239364165625525,\n",
       "  0.5238571282258341,\n",
       "  0.5238869838822482,\n",
       "  0.5238669480127066,\n",
       "  0.5238686427051895,\n",
       "  0.5238702088981845,\n",
       "  0.5238176701741033,\n",
       "  0.5238434850586008,\n",
       "  0.5238423172536324,\n",
       "  0.523792015956463,\n",
       "  0.523810069400699,\n",
       "  0.5237980796710017,\n",
       "  0.5237706632821036,\n",
       "  0.523785718892658,\n",
       "  0.5237555390263773,\n",
       "  0.5237482198123139,\n",
       "  0.5237449955526202,\n",
       "  0.5237017027441713,\n",
       "  0.5237392667147535,\n",
       "  0.5237531676504663,\n",
       "  0.5236974184851219,\n",
       "  0.52368428825341,\n",
       "  0.5236810017329946,\n",
       "  0.5237268905721425,\n",
       "  0.5236623866627922,\n",
       "  0.5237117356038182,\n",
       "  0.5236921519017915,\n",
       "  0.523638827104889,\n",
       "  0.5236925894912381,\n",
       "  0.5236640418624455,\n",
       "  0.5236248009966141,\n",
       "  0.5236351297015018,\n",
       "  0.5236115950894739,\n",
       "  0.5236189092858284,\n",
       "  0.5236146514410854,\n",
       "  0.5236243117570941,\n",
       "  0.5236099755715591,\n",
       "  0.5235565410027879,\n",
       "  0.5235490086726939,\n",
       "  0.523527524738803,\n",
       "  0.5235739665519362,\n",
       "  0.5235645296810622,\n",
       "  0.5235342708329166,\n",
       "  0.5235631087011097,\n",
       "  0.5235146151071879,\n",
       "  0.5235294232313162,\n",
       "  0.5235003590357655,\n",
       "  0.5234777583854092,\n",
       "  0.5234846315940564,\n",
       "  0.5235104245173503,\n",
       "  0.5234431218913054,\n",
       "  0.5234551800787316,\n",
       "  0.5234805479166307,\n",
       "  0.5234890099563966,\n",
       "  0.5234258700154442,\n",
       "  0.5234068558954728,\n",
       "  0.5233981354702585,\n",
       "  0.5234273263128733,\n",
       "  0.5234596422744511,\n",
       "  0.5234098572996808,\n",
       "  0.5234254985873582,\n",
       "  0.5233802993545913,\n",
       "  0.5233466244225936,\n",
       "  0.5233942326367689,\n",
       "  0.5233903715780585,\n",
       "  0.5233882758331243,\n",
       "  0.5233597217151562,\n",
       "  0.5233686969575689,\n",
       "  0.523308114288322,\n",
       "  0.5233629428475088,\n",
       "  0.5233203154523738,\n",
       "  0.5233811405212798,\n",
       "  0.5233486020335699,\n",
       "  0.5233343159682082,\n",
       "  0.5233242084727973,\n",
       "  0.5233541414073924,\n",
       "  0.5233172580618567,\n",
       "  0.5232794469197835,\n",
       "  0.5232893227164901,\n",
       "  0.5232896481291801,\n",
       "  0.5232789740113425,\n",
       "  0.5232644124789217,\n",
       "  0.5232443835713374,\n",
       "  0.523282093677759,\n",
       "  0.5232403359300468,\n",
       "  0.5232251786706664,\n",
       "  0.5232179276965335,\n",
       "  0.5232112888730228,\n",
       "  0.5232023862044994,\n",
       "  0.5232228928887344,\n",
       "  0.5232315888825934,\n",
       "  0.5232209030294486,\n",
       "  0.5232389140235029,\n",
       "  0.5232084400316016,\n",
       "  0.5231280286212873,\n",
       "  0.5231820639180316,\n",
       "  0.5232031797439328,\n",
       "  0.5231415531309361,\n",
       "  0.5232013330373325,\n",
       "  0.5231701273941101,\n",
       "  0.5231437210551424,\n",
       "  0.5231796905477861,\n",
       "  0.5231533737087484,\n",
       "  0.5231269625326851,\n",
       "  0.5231165050338643,\n",
       "  0.5230974788416434,\n",
       "  0.5230972821060191,\n",
       "  0.5231194714598147,\n",
       "  0.5231401562028992,\n",
       "  0.5230737637279339,\n",
       "  0.5231333988371537,\n",
       "  0.5230698354864115,\n",
       "  0.5230582579677243,\n",
       "  0.5231067999295672,\n",
       "  0.5230032355300932,\n",
       "  0.5230944763924588,\n",
       "  0.5230582451001695,\n",
       "  0.523065418125272,\n",
       "  0.5230774668616173,\n",
       "  0.5230645493293115,\n",
       "  0.5230408375265916,\n",
       "  0.5230960933021059,\n",
       "  0.5230391098661219,\n",
       "  0.5229787829262462,\n",
       "  0.5230236859912488,\n",
       "  0.5229730358031864,\n",
       "  0.5229509136716645,\n",
       "  0.5229623917688359,\n",
       "  0.522965402278076,\n",
       "  0.5230087627550195,\n",
       "  0.5230124647131992,\n",
       "  0.5229358389016884,\n",
       "  0.5229777703841567,\n",
       "  0.5229467476118365,\n",
       "  0.5229350270869274,\n",
       "  0.5229278282925657,\n",
       "  0.5230121094317848,\n",
       "  0.5229507929271624,\n",
       "  0.5229033264594697,\n",
       "  0.5229509357429055],\n",
       " 'val_loss': [0.61859446521697,\n",
       "  0.5755957214812922,\n",
       "  0.5660456322072968,\n",
       "  0.5611708740877911,\n",
       "  0.5573827655819373,\n",
       "  0.553489034011112,\n",
       "  0.5492368682128627,\n",
       "  0.5457870180044717,\n",
       "  0.5434913867857398,\n",
       "  0.5417053900598511,\n",
       "  0.5406398201376442,\n",
       "  0.5396930231311456,\n",
       "  0.5387845102364455,\n",
       "  0.5376911933829145,\n",
       "  0.5370608102984544,\n",
       "  0.5359077082901467,\n",
       "  0.5353168316972934,\n",
       "  0.5345842593569097,\n",
       "  0.5339727619799172,\n",
       "  0.533734321351943,\n",
       "  0.5331481532837318,\n",
       "  0.5328624607101689,\n",
       "  0.5322302817329159,\n",
       "  0.5318887960620042,\n",
       "  0.5315248496164151,\n",
       "  0.5311734710282426,\n",
       "  0.5307725832714298,\n",
       "  0.5308249343216904,\n",
       "  0.5304831910908707,\n",
       "  0.5301451375329398,\n",
       "  0.5297932295295281,\n",
       "  0.5296691513158441,\n",
       "  0.5295550113286429,\n",
       "  0.5289986077847519,\n",
       "  0.5289635595267381,\n",
       "  0.5285367970544148,\n",
       "  0.528635424327075,\n",
       "  0.5286162432616319,\n",
       "  0.5283319775651141,\n",
       "  0.5283378539531212,\n",
       "  0.528105135855636,\n",
       "  0.5281680405624514,\n",
       "  0.5276788477975178,\n",
       "  0.527974878384815,\n",
       "  0.52769118886653,\n",
       "  0.5275531134954313,\n",
       "  0.5275940262689823,\n",
       "  0.5274181441078342,\n",
       "  0.5274041089100566,\n",
       "  0.5272406644937468,\n",
       "  0.5271961223788377,\n",
       "  0.5272944344253074,\n",
       "  0.5270715111154851,\n",
       "  0.5269135551724008,\n",
       "  0.5266377785341526,\n",
       "  0.5270793246544474,\n",
       "  0.5268616664215802,\n",
       "  0.5265540095364175,\n",
       "  0.5264169425983739,\n",
       "  0.526523454887111,\n",
       "  0.5264857743329149,\n",
       "  0.5263015024545716,\n",
       "  0.5264792905105808,\n",
       "  0.5263148846665049,\n",
       "  0.5264835120216618,\n",
       "  0.5260852439616753,\n",
       "  0.5260069239430312,\n",
       "  0.5258872920904702,\n",
       "  0.5259388825757717,\n",
       "  0.5259422565378794,\n",
       "  0.5259217270990697,\n",
       "  0.525798811175959,\n",
       "  0.5256724149231019,\n",
       "  0.5260362615430259,\n",
       "  0.525650526207637,\n",
       "  0.5256599960772972,\n",
       "  0.5255035656254466,\n",
       "  0.5256709374063383,\n",
       "  0.5253979936847842,\n",
       "  0.5254425309537872,\n",
       "  0.5253591268527799,\n",
       "  0.5254556849235441,\n",
       "  0.5254518736184128,\n",
       "  0.525470542229288,\n",
       "  0.5253888230498244,\n",
       "  0.525169939529605,\n",
       "  0.5252967142477268,\n",
       "  0.5251453775700515,\n",
       "  0.5250139241296101,\n",
       "  0.5250585595282112,\n",
       "  0.5250121483473273,\n",
       "  0.5247774172604569,\n",
       "  0.5247810117112912,\n",
       "  0.5249708236717596,\n",
       "  0.5247950963373107,\n",
       "  0.5249384624686667,\n",
       "  0.524682816451158,\n",
       "  0.5246255225766965,\n",
       "  0.5246024500063764,\n",
       "  0.524626849385781,\n",
       "  0.5243404350145077,\n",
       "  0.5245468391150963,\n",
       "  0.5244284766476329,\n",
       "  0.52455361756852,\n",
       "  0.5243726741007673,\n",
       "  0.5243947392072135,\n",
       "  0.5245503560314334,\n",
       "  0.5241096550371589,\n",
       "  0.5241137316556481,\n",
       "  0.5243217114025984,\n",
       "  0.5240584510128673,\n",
       "  0.5243685230976198,\n",
       "  0.5239931783540462,\n",
       "  0.524165662081261,\n",
       "  0.5239810708577071,\n",
       "  0.5237188407076083,\n",
       "  0.5240096503156957,\n",
       "  0.5239592756682295,\n",
       "  0.5236737464017015,\n",
       "  0.5237963068776015,\n",
       "  0.5239039264558777,\n",
       "  0.5237010220686594,\n",
       "  0.5237024089669794,\n",
       "  0.5238647637813072,\n",
       "  0.5235569750874992,\n",
       "  0.523768144409831,\n",
       "  0.5236916631702485,\n",
       "  0.523563670675929,\n",
       "  0.5234027231127266,\n",
       "  0.5234833534170942,\n",
       "  0.5232531186041793,\n",
       "  0.5233610899952369,\n",
       "  0.5235098862066502,\n",
       "  0.5234470234169224,\n",
       "  0.5234307065242674,\n",
       "  0.5231043744862565,\n",
       "  0.5233612617826074,\n",
       "  0.5231142310592217,\n",
       "  0.5231651508711218,\n",
       "  0.5231371989579705,\n",
       "  0.5230253505997542,\n",
       "  0.5230418392797795,\n",
       "  0.5230219541041832,\n",
       "  0.5228871952712051,\n",
       "  0.523008381448141,\n",
       "  0.52276120704364,\n",
       "  0.5227960437778535,\n",
       "  0.5228735639796993,\n",
       "  0.5226814897079778,\n",
       "  0.5228396055659628,\n",
       "  0.5225792250982145,\n",
       "  0.5225990748986965,\n",
       "  0.5226198197380314,\n",
       "  0.5227116736454692,\n",
       "  0.5228084989679538,\n",
       "  0.5226759404186311,\n",
       "  0.5226394892708073,\n",
       "  0.5227836082136728,\n",
       "  0.5226018469992691,\n",
       "  0.5225514345537356,\n",
       "  0.5224662946491707,\n",
       "  0.5222234003912143,\n",
       "  0.5223498979235083,\n",
       "  0.5223162084575591,\n",
       "  0.5221535405976986,\n",
       "  0.5223156223452188,\n",
       "  0.5222059997116647,\n",
       "  0.5221265920294009,\n",
       "  0.522129819645145,\n",
       "  0.5221702364402089,\n",
       "  0.5220420593168678,\n",
       "  0.5218199227398973,\n",
       "  0.5221482122816691,\n",
       "  0.5222533153809183,\n",
       "  0.5220130551152113,\n",
       "  0.5218596075608478,\n",
       "  0.5218409525185097,\n",
       "  0.5218447737577485,\n",
       "  0.5217559080782944,\n",
       "  0.5218117215284487,\n",
       "  0.5218158056580924,\n",
       "  0.5220062948339353,\n",
       "  0.5218736524504375,\n",
       "  0.5219110374043627,\n",
       "  0.5217505986612987,\n",
       "  0.5216451805781542,\n",
       "  0.5219248947573871,\n",
       "  0.5218509367811002,\n",
       "  0.5217718876958862,\n",
       "  0.5217972017400633,\n",
       "  0.5218073391332859,\n",
       "  0.5216790808410179,\n",
       "  0.5216496310582975,\n",
       "  0.5216372143931505,\n",
       "  0.5215679474962436,\n",
       "  0.5218623971066824,\n",
       "  0.5214925875993279,\n",
       "  0.5216962809000558,\n",
       "  0.5218226405663219,\n",
       "  0.5212386148731883,\n",
       "  0.5212683391764881,\n",
       "  0.5214167423849183,\n",
       "  0.5213427715669803,\n",
       "  0.5213381471188088,\n",
       "  0.5217178178027393,\n",
       "  0.5217016978961665,\n",
       "  0.5212904856941565,\n",
       "  0.5214750037445286,\n",
       "  0.5213666335354007,\n",
       "  0.5212357366472725,\n",
       "  0.5214105224221702,\n",
       "  0.5209603629461149,\n",
       "  0.5215984539287847,\n",
       "  0.521316106241893,\n",
       "  0.5216381683097622,\n",
       "  0.5213881641384063,\n",
       "  0.521464612910418,\n",
       "  0.5214085249396844,\n",
       "  0.5214287471480485,\n",
       "  0.5211632106362319,\n",
       "  0.5211939101781302,\n",
       "  0.5212699422022191,\n",
       "  0.5211448393216948,\n",
       "  0.5212253392711887,\n",
       "  0.5212453213649068,\n",
       "  0.5212568788509059,\n",
       "  0.5210396526305656,\n",
       "  0.521028561078436,\n",
       "  0.5212040589592322,\n",
       "  0.5212189746581442,\n",
       "  0.5211977617042821,\n",
       "  0.5212384830645429,\n",
       "  0.521051058924295,\n",
       "  0.5206780777714117,\n",
       "  0.5208972273318748,\n",
       "  0.5208463065507936,\n",
       "  0.5208136991756719,\n",
       "  0.5210902390441274,\n",
       "  0.5209864944946475,\n",
       "  0.5209211690154502,\n",
       "  0.5208595046182958,\n",
       "  0.5208263775197471,\n",
       "  0.5209874774866957,\n",
       "  0.5207772649885193,\n",
       "  0.5208392848328847,\n",
       "  0.5210120794249744,\n",
       "  0.5209316560407964,\n",
       "  0.520908520231402,\n",
       "  0.520738865301861,\n",
       "  0.5207968870314156,\n",
       "  0.5207484485657234,\n",
       "  0.5209494119252616,\n",
       "  0.5205523681834461,\n",
       "  0.5207000566207296,\n",
       "  0.5204762874580011,\n",
       "  0.5208922895958753,\n",
       "  0.5206986152059664,\n",
       "  0.5204355302380352,\n",
       "  0.5208017503827568,\n",
       "  0.5206482887752657,\n",
       "  0.5204595698089134,\n",
       "  0.520561906380382,\n",
       "  0.52046767773667,\n",
       "  0.520476961765832,\n",
       "  0.5206718994834558,\n",
       "  0.520659250941703,\n",
       "  0.5203818906613482,\n",
       "  0.5205280710526599,\n",
       "  0.5206455314547066,\n",
       "  0.5203532273691844,\n",
       "  0.5206641043589367,\n",
       "  0.5203700785229846,\n",
       "  0.5204921405974442,\n",
       "  0.5204857723499702,\n",
       "  0.520410404215014,\n",
       "  0.5203965294167279,\n",
       "  0.5203201334650923,\n",
       "  0.5204121000398465,\n",
       "  0.5202903255699126,\n",
       "  0.5202360531178917,\n",
       "  0.520281065770281,\n",
       "  0.5202737770429472,\n",
       "  0.5202056751018618,\n",
       "  0.5202601561701394,\n",
       "  0.5203119916159932,\n",
       "  0.5203096239062829,\n",
       "  0.5204222192609214,\n",
       "  0.5202344450039592,\n",
       "  0.5203719773912817,\n",
       "  0.5203977505850598,\n",
       "  0.5201407796968289,\n",
       "  0.5201656503405997,\n",
       "  0.5201063883013841,\n",
       "  0.5201190133404926,\n",
       "  0.5201745379746445,\n",
       "  0.520182197898384,\n",
       "  0.5200038513516992,\n",
       "  0.5202774015383992,\n",
       "  0.5199764559908611,\n",
       "  0.5200341053125335,\n",
       "  0.5200532517297481,\n",
       "  0.5201895302388726,\n",
       "  0.5200188283513232,\n",
       "  0.5200571202165712,\n",
       "  0.5199387114707047,\n",
       "  0.5199557911089765,\n",
       "  0.52016294826337,\n",
       "  0.5202210332319989,\n",
       "  0.5202319125334421,\n",
       "  0.5199732254675733,\n",
       "  0.5198208243866277,\n",
       "  0.5199380284402428,\n",
       "  0.5200501130363806,\n",
       "  0.520058687140302,\n",
       "  0.5198172321164511,\n",
       "  0.5199621821322092,\n",
       "  0.5196675680032591,\n",
       "  0.5200155888631092,\n",
       "  0.5196252000041124,\n",
       "  0.5196838878034576,\n",
       "  0.5199127192419719,\n",
       "  0.5201780355073572,\n",
       "  0.5197784202854808,\n",
       "  0.5197963537724037,\n",
       "  0.5198461735636238,\n",
       "  0.5199454639985309,\n",
       "  0.5197071525139537,\n",
       "  0.5196930865446726,\n",
       "  0.5194763102667118,\n",
       "  0.5196100894997759,\n",
       "  0.5194800801393462,\n",
       "  0.5196457648180365,\n",
       "  0.5197662161133154,\n",
       "  0.5193790478434989,\n",
       "  0.5196465011534652,\n",
       "  0.5198669462669187,\n",
       "  0.519705568144961,\n",
       "  0.5195403416466907,\n",
       "  0.5194078715351539,\n",
       "  0.5195353801657514,\n",
       "  0.5194122667719678,\n",
       "  0.5195726255575815,\n",
       "  0.5194668631728102,\n",
       "  0.5197876650628036,\n",
       "  0.5195230417619876,\n",
       "  0.5196380583735986,\n",
       "  0.5196910217040922,\n",
       "  0.5195554533625036,\n",
       "  0.5196289958023443,\n",
       "  0.5195363740610882,\n",
       "  0.5194554716590943,\n",
       "  0.5195632305571704,\n",
       "  0.5197635256662602,\n",
       "  0.5195402379443006,\n",
       "  0.5193879984743227,\n",
       "  0.5196142470448967,\n",
       "  0.5196117342002993,\n",
       "  0.5192550466797217,\n",
       "  0.519286815228501,\n",
       "  0.5193579647599197,\n",
       "  0.5193757363935796,\n",
       "  0.5193725390647485,\n",
       "  0.5197394931219458,\n",
       "  0.5193155519361419,\n",
       "  0.5192895156096636,\n",
       "  0.5194810660389381,\n",
       "  0.5195564821483644,\n",
       "  0.5192969000436426,\n",
       "  0.519431007102253,\n",
       "  0.5193296142709933,\n",
       "  0.5191463639096516,\n",
       "  0.5192630581739472,\n",
       "  0.5194053179849454,\n",
       "  0.519573430947172,\n",
       "  0.5193184841939105,\n",
       "  0.5193876611992596,\n",
       "  0.5196548513765258,\n",
       "  0.5193487563268925,\n",
       "  0.5193971533600877,\n",
       "  0.5192397726260549,\n",
       "  0.5196391440988556,\n",
       "  0.5190902269952665,\n",
       "  0.5190633145289693,\n",
       "  0.5192839626858874,\n",
       "  0.5193467149889566,\n",
       "  0.5194149034294656,\n",
       "  0.5195639051072966,\n",
       "  0.5193290291278343,\n",
       "  0.5194049060829287,\n",
       "  0.5190710951157702,\n",
       "  0.5192514493213436,\n",
       "  0.5189850921553325,\n",
       "  0.519183748136691,\n",
       "  0.5192498971776265,\n",
       "  0.519296866849186,\n",
       "  0.5189289966734444,\n",
       "  0.5193309684594473,\n",
       "  0.5191979602100404,\n",
       "  0.5191134902519908,\n",
       "  0.5193561414877573,\n",
       "  0.5194866853516277,\n",
       "  0.5190632917532106,\n",
       "  0.5190041884174191,\n",
       "  0.5191023596903173,\n",
       "  0.5191816375022982,\n",
       "  0.5192222263270277,\n",
       "  0.518707855687878,\n",
       "  0.5193126523882393,\n",
       "  0.5190061279913274,\n",
       "  0.5190105796829472,\n",
       "  0.5192107061545054,\n",
       "  0.5190623659428543,\n",
       "  0.5187199568845392,\n",
       "  0.5192647493951689,\n",
       "  0.5189881036436654,\n",
       "  0.5191411545606164,\n",
       "  0.518898474734004,\n",
       "  0.5190470458530798,\n",
       "  0.5189504349619393,\n",
       "  0.5191103069762874,\n",
       "  0.5187239341619538,\n",
       "  0.5188564864115986,\n",
       "  0.5190945744999056,\n",
       "  0.5187195965914222,\n",
       "  0.5190318536952259,\n",
       "  0.5190282621519352,\n",
       "  0.5187889650100614,\n",
       "  0.5189844573416361,\n",
       "  0.519247068622248,\n",
       "  0.5189839395565715,\n",
       "  0.5190474112343982,\n",
       "  0.5191732850985799,\n",
       "  0.5186516012602705,\n",
       "  0.5186354954068254,\n",
       "  0.5191452917529316,\n",
       "  0.5191019710486497,\n",
       "  0.5187654284442343,\n",
       "  0.5188120647174556,\n",
       "  0.5190396863755172,\n",
       "  0.5189127565884009,\n",
       "  0.5188598843609414,\n",
       "  0.5188553195174147,\n",
       "  0.5189290136341157,\n",
       "  0.5188934039778825,\n",
       "  0.5190900137753991,\n",
       "  0.5188440460015119,\n",
       "  0.5190064626011422,\n",
       "  0.5187622122163695,\n",
       "  0.5188233072195596,\n",
       "  0.5188070758571469,\n",
       "  0.519089282770467,\n",
       "  0.5189223553591628,\n",
       "  0.5189009727985878,\n",
       "  0.5188061515005623,\n",
       "  0.5187616927352378,\n",
       "  0.5188426600723732,\n",
       "  0.5187203048205957,\n",
       "  0.5188989166806384,\n",
       "  0.5186543542195142,\n",
       "  0.5185978657346431,\n",
       "  0.5190267989305946,\n",
       "  0.5186563458869128,\n",
       "  0.5190752572645017,\n",
       "  0.5187722495416316,\n",
       "  0.5187555464302621,\n",
       "  0.518709825306404,\n",
       "  0.5187830072108323,\n",
       "  0.5185556792146792,\n",
       "  0.5185211484994345,\n",
       "  0.5186139015647454,\n",
       "  0.5186868120984334,\n",
       "  0.5187492782507486,\n",
       "  0.5189066282132777,\n",
       "  0.5188202017206487,\n",
       "  0.5186020101958174,\n",
       "  0.5188591376068147,\n",
       "  0.5189107690400224,\n",
       "  0.5186057955753512,\n",
       "  0.5188897378076383,\n",
       "  0.5188330775353966,\n",
       "  0.5188635677341523,\n",
       "  0.5190095513816771,\n",
       "  0.5186025485759829,\n",
       "  0.5186183668249021,\n",
       "  0.5184469077645278,\n",
       "  0.5185650281789826,\n",
       "  0.5188350418234259,\n",
       "  0.5186797722083766,\n",
       "  0.5187345210129652,\n",
       "  0.5185756632467595,\n",
       "  0.5183235649170914,\n",
       "  0.5186077080121855,\n",
       "  0.5185520864599119,\n",
       "  0.5186579922835032,\n",
       "  0.5187334110581778,\n",
       "  0.5185081689822965,\n",
       "  0.5187171007074961,\n",
       "  0.5186360238528833,\n",
       "  0.5184980296507115,\n",
       "  0.5185635470277895]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_callback.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAC5VJREFUeJzt3V2IXPUdxvHn2Ul8zwvYGCUJjYIEpFAjISABobEtsYr2ohcJKFQKuVK0FkR750VvxV5UQaJWMFXaqCBitYKKFVprEtPWuLGkwZJttKstedFW92V+vdiJ3SRr9+zOOf8z++P7gSU7u8P+n5PdZ8+Z2TPn54gQgJyG2g4AoDkUHEiMggOJUXAgMQoOJEbBgcQoOJAYBQcSo+BAYoua+KLLliyLlSsubuJLn6HkmXjRLXvWX7fbLbaWi60k2QVXGyq5ZeWM/nNUx08cm3XjGin4yhUX68GfPNjElz7D2OflSvDZf8aKrTW13mfF1mrkB+HL1lpU7sBx0bmLi601tWCZZe6674eV7schOpAYBQcSo+BAYhQcSIyCA4lRcCAxCg4kRsGBxCoV3PYW2+/ZPmj7nqZDAajHrAW33ZH0M0nXSbpC0jbbVzQdDED/quzBN0o6GBGHImJM0lOSbmo2FoA6VCn4KkmHp90e6X0MwICrUvCZXrFyxsuqbG+3vdv27mMnjvafDEDfqhR8RNKaabdXSzpy+p0i4uGI2BARG5YtWV5XPgB9qFLwtyRdbvtS22dJ2irpuWZjAajDrK9ejYgJ27dJeklSR9KjEbG/8WQA+lbp5ekR8YKkFxrOAqBmnMkGJEbBgcQoOJAYBQcSo+BAYhQcSIyCA4lRcCCxRuYw2FZn6KwmvvQZhsbGi6wjSRorOwYnPpssttZ4FByTdHa5OSqL3Cm2liSdfU6Zn5Gqe2b24EBiFBxIjIIDiVFwIDEKDiRGwYHEKDiQGAUHEqPgQGJVJps8anvU9jslAgGoT5U9+M8lbWk4B4AGzFrwiHhd0r8KZAFQMx6DA4nVVvBTRhcdZ3QRMAhqK/gpo4uWMroIGAQcogOJVfkz2ZOSfidpne0R2z9oPhaAOlSZTbatRBAA9eMQHUiMggOJUXAgMQoOJEbBgcQoOJAYBQcSo+BAYs3MkIneWwHdbrmRO93uRLG1JGl8suB6BUcXdbrl9isFN0uSNDQ0WPvMwUoDoFYUHEiMggOJUXAgMQoOJEbBgcQoOJAYBQcSo+BAYhQcSKzKRRfX2H7V9rDt/bbvKBEMQP+qnIs+IelHEbHX9hJJe2y/HBHvNpwNQJ+qzCb7ICL29t4/IWlY0qqmgwHo35weg9teK2m9pDdn+Nz/RhedYHQRMAgqF9z2BZKelnRnRBw//fOnjC5awugiYBBUKrjtxZoq986IeKbZSADqUuVZdEt6RNJwRNzffCQAdamyB98k6RZJm23v6719p+FcAGpQZTbZG5JcIAuAmnEmG5AYBQcSo+BAYhQcSIyCA4lRcCAxCg4kRsGBxBqZTWZJiwv97pgYKjhTa2iy2FqS1PF4ucWi3LYtKnreVDPj976M1Sm63mzYgwOJUXAgMQoOJEbBgcQoOJAYBQcSo+BAYhQcSIyCA4lVuejiObb/YPuPvdFF95UIBqB/Vc7j+1zS5oj4pHf55Dds/zoift9wNgB9qnLRxZD0Se/m4t5bNBkKQD2qDj7o2N4naVTSyxHxf0cXHWV0ETAQKhU8IiYj4kpJqyVttP21Ge7zxeii5YwuAgbCnJ5Fj4ijkl6TtKWRNABqVeVZ9BW2l/feP1fSNyUdaDoYgP5VeRb9EkmP2+5o6hfCLyPi+WZjAahDlWfR/6SpmeAAFhjOZAMSo+BAYhQcSIyCA4lRcCAxCg4kRsGBxCg4kFhDc11CijJjd7oxUWQdSXKUG5MkFX5NbrfctkXBMUmdwt8zF/p/rDr8iT04kBgFBxKj4EBiFBxIjIIDiVFwIDEKDiRGwYHEKDiQWOWC966N/rZtrscGLBBz2YPfIWm4qSAA6ld1sslqSddL2tFsHAB1qroHf0DS3ZLKnrkPoC9VBh/cIGk0IvbMcr9ps8mO1RYQwPxV2YNvknSj7fclPSVps+0nTr/TqbPJltUcE8B8zFrwiLg3IlZHxFpJWyW9EhE3N54MQN/4OziQ2Jyu6BIRr2lquiiABYA9OJAYBQcSo+BAYhQcSIyCA4lRcCAxCg4kRsGBxBoZXRSSxifLjKfpqtzoom633MgdSeqo3HqTQ+UGJUXBmUxRcLSVJI1H1aFC/YmKI5nYgwOJUXAgMQoOJEbBgcQoOJAYBQcSo+BAYhQcSIyCA4lVOpOtd0XVE5ImJU1ExIYmQwGox1xOVf1GRHzcWBIAteMQHUisasFD0m9s77G9vclAAOpT9RB9U0QcsX2RpJdtH4iI16ffoVf87ZK08sKLao4JYD4q7cEj4kjv31FJz0raOMN9vhhdtGzp8npTApiXKsMHz7e95OT7kr4t6Z2mgwHoX5VD9JWSnrV98v6/iIgXG00FoBazFjwiDkn6eoEsAGrGn8mAxCg4kBgFBxKj4EBiFBxIjIIDiVFwIDEKDiTWyOgiKdQtNHZnsltuDo6j7OiirguOE+pWG4VTh1I/G5I0obFia0mSJ8vsM6v+ZLAHBxKj4EBiFBxIjIIDiVFwIDEKDiRGwYHEKDiQGAUHEqtUcNvLbe+yfcD2sO2rmw4GoH9VT1X9qaQXI+J7ts+SdF6DmQDUZNaC214q6RpJ35ekiBiTCp/gC2BeqhyiXybpI0mP2X7b9o7e9dEBDLgqBV8k6SpJD0XEekmfSrrn9DvZ3m57t+3dR48fqzkmgPmoUvARSSMR8Wbv9i5NFf4U00cXLV+6rM6MAOZp1oJHxIeSDtte1/vQtZLebTQVgFpUfRb9dkk7e8+gH5J0a3ORANSlUsEjYp+kDQ1nAVAzzmQDEqPgQGIUHEiMggOJUXAgMQoOJEbBgcQoOJAYBQcSa2Q2WSg0OVToJeOl1pE0ubjs78POeMn1GhpTN4Nup9x2jRdbqadTZsWoOJ2MPTiQGAUHEqPgQGIUHEiMggOJUXAgMQoOJEbBgcQoOJDYrAW3vc72vmlvx23fWSIcgP7Men5iRLwn6UpJst2R9HdJzzacC0AN5nqIfq2kv0bE35oIA6Becy34VklPzvSJ6aOLjp1gdBEwCCoXvDf04EZJv5rp89NHFy1bwugiYBDMZQ9+naS9EfGPpsIAqNdcCr5NX3J4DmAwVSq47fMkfUvSM83GAVCnqrPJ/i3pwoazAKgZZ7IBiVFwIDEKDiRGwYHEKDiQGAUHEqPgQGIUHEjMEdVGoMzpi9ofSZrrS0q/Iunj2sMMhqzbxna156sRsWK2OzVS8PmwvTsiNrSdowlZt43tGnwcogOJUXAgsUEq+MNtB2hQ1m1juwbcwDwGB1C/QdqDA6jZQBTc9hbb79k+aPuetvPUwfYa26/aHra93/YdbWeqk+2O7bdtP992ljrZXm57l+0Dve/d1W1n6kfrh+i9a63/RVNXjBmR9JakbRHxbqvB+mT7EkmXRMRe20sk7ZH03YW+XSfZvkvSBklLI+KGtvPUxfbjkn4bETt6Fxo9LyKOtp1rvgZhD75R0sGIOBQRY5KeknRTy5n6FhEfRMTe3vsnJA1LWtVuqnrYXi3pekk72s5SJ9tLJV0j6RFJioixhVxuaTAKvkrS4Wm3R5SkCCfZXitpvaQ3201Smwck3S2p23aQml0m6SNJj/UefuywfX7bofoxCAX3DB9L89S+7QskPS3pzog43naeftm+QdJoROxpO0sDFkm6StJDEbFe0qeSFvRzQoNQ8BFJa6bdXi3pSEtZamV7sabKvTMislyRdpOkG22/r6mHU5ttP9FupNqMSBqJiJNHWrs0VfgFaxAK/paky21f2ntSY6uk51rO1Dfb1tRjueGIuL/tPHWJiHsjYnVErNXU9+qViLi55Vi1iIgPJR22va73oWslLegnRStdNrlJETFh+zZJL0nqSHo0Iva3HKsOmyTdIunPtvf1PvbjiHihxUyY3e2SdvZ2Nock3dpynr60/mcyAM0ZhEN0AA2h4EBiFBxIjIIDiVFwIDEKDiRGwYHEKDiQ2H8BXQnmElxAXtgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_directory = base_dir + '/tiny_test16/class0'\n",
    "files = os.listdir(images_directory)\n",
    "files.sort()\n",
    "\n",
    "images = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for file in files:\n",
    "    counter += 1\n",
    "    if counter > 100:\n",
    "        break\n",
    "    if not file.startswith('.'):\n",
    "        img = imageio.imread(images_directory + '/' + file)\n",
    "        img = img[4: 12, 4: 12, :]\n",
    "        img = np.expand_dims(img, axis=-1)        \n",
    "        images.append(img)\n",
    "\n",
    "\n",
    "images = np.array(images).reshape(np.array(images).shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "images = images / 255\n",
    "predictions = autoencoder.predict_on_batch(np.array(images))\n",
    "print(\"predictions: \")\n",
    "for i, im1 in enumerate(images):\n",
    "    im_1 = im1.reshape(input_shape)\n",
    "    plt.imshow(im_1, interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "    pred_1 = predictions[i].numpy()#.reshape(input_shape)\n",
    "    plt.imshow(pred_1, interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "    if i == 2:\n",
    "        break\n",
    "    print(\"next\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f4b48aa48d0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f4b4dd34d68>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f4b48a0eb70>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f4b48a0e1d0>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f4b489dd940>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f4b489ddcc0>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 8, 8, 3)]         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 8, 8, 32)          896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 32)          0         \n",
      "=================================================================\n",
      "Total params: 19,392\n",
      "Trainable params: 19,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = Model(input_img, encoded)\n",
    "for i in range(1, len(encoder.layers)):\n",
    "    print(encoder.get_layer(index=i))\n",
    "    encoder.get_layer(index=i).set_weights(autoencoder.get_layer(index=i).get_weights())\n",
    "encoder.summary()\n",
    "\n",
    "# encoder.save(base_dir + '/encoder' + model_version + '.h5')\n",
    "encoder.save(base_dir + '/' + model_version + '__encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = encoder.predict_on_batch(np.array(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 32), dtype=float32, numpy=\n",
       "array([[[ 0.23345804,  0.13077457,  0.23456451,  0.17916967,\n",
       "          0.2824274 ,  0.05104627,  0.09579747,  0.07939498,\n",
       "          0.0850114 ,  0.06821102,  0.03490557,  0.25148502,\n",
       "          0.03727317,  0.12141401,  0.35879073,  0.09728938,\n",
       "          0.10271734,  0.08301196, -0.03964257,  0.16273871,\n",
       "          0.02464434,  0.1567319 ,  0.14562167,  0.18703483,\n",
       "          0.15531506,  0.19795409, -0.04767317, -0.00051337,\n",
       "          0.2275063 ,  0.01976364,  0.21862242,  0.16393398]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3005839"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(predictions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.157861"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(predictions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 1, 1, 32])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 103704<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.91MB of 0.91MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>wandb/run-20201030_140320-kfpgf6u4/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>wandb/run-20201030_140320-kfpgf6u4/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>499</td></tr><tr><td>loss</td><td>0.52295</td></tr><tr><td>val_loss</td><td>0.51856</td></tr><tr><td>_step</td><td>499</td></tr><tr><td>_runtime</td><td>22961</td></tr><tr><td>_timestamp</td><td>1604085961</td></tr><tr><td>best_val_loss</td><td>0.51832</td></tr><tr><td>best_epoch</td><td>490</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1501 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">dreadful-bones-7</strong>: <a href=\"https://wandb.ai/nimpy/patch-desc-ae/runs/kfpgf6u4\" target=\"_blank\">https://wandb.ai/nimpy/patch-desc-ae/runs/kfpgf6u4</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 46., 136., 939., 769., 501., 345., 266., 145.,  37.,  16.]),\n",
       " array([-0.3005839 , -0.15473941, -0.00889492,  0.13694957,  0.28279406,\n",
       "         0.42863855,  0.57448304,  0.7203275 ,  0.866172  ,  1.0120165 ,\n",
       "         1.157861  ], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(predictions.numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(history_callback.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb run dreadful-bones-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
