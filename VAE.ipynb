{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os import listdir\n",
    "from os import system\n",
    "import os\n",
    "import random\n",
    "\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 56\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/scratch/image_datasets/2_for_learned_brief/ready'\n",
    "\n",
    "train_data_dir      = base_dir + '/train'\n",
    "validation_data_dir = base_dir + '/validation'\n",
    "test_data_dir       = base_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data(dir_patches):\n",
    "    \"\"\"Load all the patches from dir_patches into tensors for training the autoencoder.\n",
    "    Return:\n",
    "        patches  -- tensor of stacked patches    \n",
    "    \"\"\"\n",
    "    files_patches = listdir(dir_patches + '/class0')\n",
    "    files_patches.sort()\n",
    "    \n",
    "    patches = []\n",
    "    \n",
    "    for file_patch in files_patches:\n",
    "        patch = imageio.imread(dir_patches + '/class0/' + file_patch)\n",
    "        \n",
    "        patches.append(patch)\n",
    "        \n",
    "    patches = np.array(patches)\n",
    "    patches = patches.astype(np.float64) / 255\n",
    "    patches = np.expand_dims(patches, -1)\n",
    "        \n",
    "    print(\"patches shape:\", patches.shape)\n",
    "    \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patches shape: (64598, 56, 56, 1)\n",
      "patches shape: (8075, 56, 56, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = loading_data(train_data_dir)\n",
    "x_validation = loading_data(validation_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 56, 56, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 28, 28, 32)   320         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 14, 14, 64)   18496       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 7, 7, 64)     36928       conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 3136)         0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          803072      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 128)          32896       dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 128)          32896       dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sampling_2 (Sampling)           (None, 128)          0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 924,608\n",
      "Trainable params: 924,608\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(patch_size, patch_size, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3136)              404544    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_12 (Conv2DT (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_13 (Conv2DT (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_14 (Conv2DT (None, 56, 56, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_15 (Conv2DT (None, 56, 56, 1)         289       \n",
      "=================================================================\n",
      "Total params: 497,153\n",
      "Trainable params: 497,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = encoder(data)\n",
    "            reconstruction = decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                keras.losses.binary_crossentropy(data, reconstruction)\n",
    "            )\n",
    "            reconstruction_loss *= 28 * 28\n",
    "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *= -0.5\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(x_train, epochs=50, batch_size=batch_size)\n",
    "\n",
    "\n",
    "model_version = \"0.0.0.1\"\n",
    "vae.save_weights(base_dir + '/vae_' + model_version + '.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "199/504 [==========>...................] - ETA: 3s - loss: 460.2773 - reconstruction_loss: 458.7855 - kl_loss: 1.4918WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/504 [==========>...................] - 2s 11ms/step - loss: 460.3369 - reconstruction_loss: 458.8441 - kl_loss: 1.4929\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "# vae.fit(x_train, epochs=30, batch_size=128)\n",
    "# vae.fit_generator(image_datagen.flow(x_train, x_train, batch_size=batch_size), steps_per_epoch=x_train.shape[0], epochs=50)\n",
    "vae.fit(x_train, steps_per_epoch=x_train.shape[0], epochs=50, batch_size=batch_size)\n",
    "\n",
    "model_version = \"0.0.0.1\"\n",
    "vae.save_weights(base_dir + '/vae_' + model_version + '.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 460.6512 - reconstruction_loss: 459.5046 - kl_loss: 1.1465\n",
      "Epoch 2/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 426.6239 - reconstruction_loss: 424.8351 - kl_loss: 1.7887\n",
      "Epoch 3/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 423.1027 - reconstruction_loss: 421.3390 - kl_loss: 1.7637\n",
      "Epoch 4/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 421.4314 - reconstruction_loss: 419.6671 - kl_loss: 1.7643\n",
      "Epoch 5/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 420.3955 - reconstruction_loss: 418.6388 - kl_loss: 1.7567\n",
      "Epoch 6/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 419.7329 - reconstruction_loss: 417.9953 - kl_loss: 1.7376\n",
      "Epoch 7/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 419.2359 - reconstruction_loss: 417.5242 - kl_loss: 1.7117\n",
      "Epoch 8/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 418.7825 - reconstruction_loss: 417.0877 - kl_loss: 1.6947\n",
      "Epoch 9/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 418.5029 - reconstruction_loss: 416.8247 - kl_loss: 1.6782\n",
      "Epoch 10/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 418.1853 - reconstruction_loss: 416.5235 - kl_loss: 1.6619\n",
      "Epoch 11/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 417.9953 - reconstruction_loss: 416.3472 - kl_loss: 1.6481\n",
      "Epoch 12/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 417.7877 - reconstruction_loss: 416.1524 - kl_loss: 1.6353\n",
      "Epoch 13/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 417.5747 - reconstruction_loss: 415.9494 - kl_loss: 1.6253\n",
      "Epoch 14/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 417.4115 - reconstruction_loss: 415.7966 - kl_loss: 1.6150\n",
      "Epoch 15/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 417.1836 - reconstruction_loss: 415.5774 - kl_loss: 1.6062\n",
      "Epoch 16/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 417.1116 - reconstruction_loss: 415.5145 - kl_loss: 1.5971\n",
      "Epoch 17/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 417.0394 - reconstruction_loss: 415.4527 - kl_loss: 1.5867\n",
      "Epoch 18/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 416.9380 - reconstruction_loss: 415.3593 - kl_loss: 1.5787\n",
      "Epoch 19/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 416.8706 - reconstruction_loss: 415.2967 - kl_loss: 1.5739\n",
      "Epoch 20/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 416.7122 - reconstruction_loss: 415.1408 - kl_loss: 1.5714\n",
      "Epoch 21/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 416.6722 - reconstruction_loss: 415.1064 - kl_loss: 1.5658\n",
      "Epoch 22/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 416.5058 - reconstruction_loss: 414.9437 - kl_loss: 1.5621\n",
      "Epoch 23/50\n",
      "505/505 [==============================] - 5s 11ms/step - loss: 416.5372 - reconstruction_loss: 414.9797 - kl_loss: 1.5575\n",
      "Epoch 24/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 416.4321 - reconstruction_loss: 414.8773 - kl_loss: 1.5548\n",
      "Epoch 25/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 416.4159 - reconstruction_loss: 414.8630 - kl_loss: 1.5530\n",
      "Epoch 26/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 416.2948 - reconstruction_loss: 414.7437 - kl_loss: 1.5510\n",
      "Epoch 27/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 416.2209 - reconstruction_loss: 414.6732 - kl_loss: 1.5477\n",
      "Epoch 28/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 416.2380 - reconstruction_loss: 414.6922 - kl_loss: 1.5458\n",
      "Epoch 29/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 416.1457 - reconstruction_loss: 414.6005 - kl_loss: 1.5453\n",
      "Epoch 30/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 416.1291 - reconstruction_loss: 414.5855 - kl_loss: 1.5436\n",
      "Epoch 31/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 416.0670 - reconstruction_loss: 414.5255 - kl_loss: 1.5415\n",
      "Epoch 32/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 416.0156 - reconstruction_loss: 414.4759 - kl_loss: 1.5396\n",
      "Epoch 33/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.9825 - reconstruction_loss: 414.4444 - kl_loss: 1.5380\n",
      "Epoch 34/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.9699 - reconstruction_loss: 414.4336 - kl_loss: 1.5363\n",
      "Epoch 35/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.8984 - reconstruction_loss: 414.3630 - kl_loss: 1.5354\n",
      "Epoch 36/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.8260 - reconstruction_loss: 414.2910 - kl_loss: 1.5350\n",
      "Epoch 37/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.7799 - reconstruction_loss: 414.2460 - kl_loss: 1.5339\n",
      "Epoch 38/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.8716 - reconstruction_loss: 414.3381 - kl_loss: 1.5335\n",
      "Epoch 39/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.7776 - reconstruction_loss: 414.2433 - kl_loss: 1.5343\n",
      "Epoch 40/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.7748 - reconstruction_loss: 414.2416 - kl_loss: 1.5332\n",
      "Epoch 41/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.6971 - reconstruction_loss: 414.1629 - kl_loss: 1.5342\n",
      "Epoch 42/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.6475 - reconstruction_loss: 414.1128 - kl_loss: 1.5347\n",
      "Epoch 43/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.6408 - reconstruction_loss: 414.1058 - kl_loss: 1.5350\n",
      "Epoch 44/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.6068 - reconstruction_loss: 414.0711 - kl_loss: 1.5357\n",
      "Epoch 45/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.6325 - reconstruction_loss: 414.0989 - kl_loss: 1.5336\n",
      "Epoch 46/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.5517 - reconstruction_loss: 414.0154 - kl_loss: 1.5363\n",
      "Epoch 47/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.5157 - reconstruction_loss: 413.9809 - kl_loss: 1.5348\n",
      "Epoch 48/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.4557 - reconstruction_loss: 413.9189 - kl_loss: 1.5368\n",
      "Epoch 49/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.4706 - reconstruction_loss: 413.9335 - kl_loss: 1.5371\n",
      "Epoch 50/50\n",
      "505/505 [==============================] - 6s 11ms/step - loss: 415.4656 - reconstruction_loss: 413.9281 - kl_loss: 1.5375\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "# vae.fit(x_train, epochs=30, batch_size=128)\n",
    "# vae.fit_generator(image_datagen.flow(x_train, x_train, batch_size=batch_size), steps_per_epoch=x_train.shape[0], epochs=50)\n",
    "vae.fit(x_train, epochs=50, batch_size=batch_size)\n",
    "\n",
    "model_version = \"0.0.0.1\"\n",
    "vae.save_weights(base_dir + '/vae_' + model_version + '.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_2_2",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
